{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:28:59.154368100Z",
     "start_time": "2024-02-01T17:28:31.272055500Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 12:28:43.625583: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-01 12:28:43.680182: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-01 12:28:43.680232: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-01 12:28:43.682207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-01 12:28:43.693093: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-01 12:28:43.694537: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-01 12:28:48.119147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os; ROOT = '/fs01/home/afallah/odyssey/slurm'; os.chdir(ROOT)\n",
    "import sys\n",
    "import scipy, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, precision_recall_curve, roc_auc_score, average_precision_score\n",
    "from scipy.sparse import csr_matrix, hstack, vstack, save_npz, load_npz\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import relu, leaky_relu, sigmoid\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from models.cehr_bert.data import PretrainDataset, FinetuneDataset\n",
    "from models.cehr_bert.model import BertPretrain\n",
    "# from models.cehr_bert.tokenizer import ConceptTokenizer\n",
    "from models.cehr_bert.embeddings import Embeddings\n",
    "\n",
    "import glob, json, random, glob\n",
    "from random import randint\n",
    "from typing import Sequence, Union\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, decoders, models, normalizers, pre_tokenizers, processors, trainers\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_ROOT = f'{ROOT}/data'\n",
    "DATA_PATH = f'{DATA_ROOT}/patient_sequences.parquet'\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[REG]\", \"[MASK]\", \"[VS]\", \"[VE]\"] +\\\n",
    "                 [f'[W_{i}]' for i in range(0, 4)] + [f'[M_{i}]' for i in range(0, 13)] + ['[LT]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:28:59.162349500Z",
     "start_time": "2024-02-01T17:28:59.152348300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    seed = 23\n",
    "    data_dir = DATA_ROOT\n",
    "    test_size = 0.2\n",
    "    batch_size = 64\n",
    "    num_workers = 2\n",
    "    vocab_size = None\n",
    "    embedding_size = 128\n",
    "    time_embeddings_size = 16\n",
    "    max_len = 512\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:28:59.224348200Z",
     "start_time": "2024-02-01T17:28:59.162349500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# data = pd.read_parquet(DATA_PATH)\n",
    "# data.rename(columns={'event_tokens': 'event_tokens_untruncated', 'event_tokens_updated': 'event_tokens'}, inplace=True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:28:59.243346600Z",
     "start_time": "2024-02-01T17:28:59.184350500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>procedure</th>\n",
       "      <th>lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[CLS], 02H64JZ, 8222, [VS], 100, 0JC00ZZ, 100...</td>\n",
       "      <td>[[VS], 51079017220, 00069419068, 63323027205, ...</td>\n",
       "      <td>[[VS], 51044_3, 51429_0, 52301_3, 52178_0, 509...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[CLS], 8222, 8127, [VS], 8127, [VE], [LT], [R...</td>\n",
       "      <td>[[VS], 00078059620, 00641601310, [VE], [LT], [...</td>\n",
       "      <td>[[VS], 51044_3, 50908_4, 50908_4, 50841_4, [VE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[CLS], 0JC00ZZ, 0YHM0YZ, [VS], 8222, 7906, [V...</td>\n",
       "      <td>[[VS], 00078059620, 68084061221, [VE], [W_3], ...</td>\n",
       "      <td>[[VS], 51009_4, 51564_0, 50995_4, 50995_4, [VE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[CLS], 0JC00ZZ, 8127, [VS], 9652, 0JC00ZZ, 81...</td>\n",
       "      <td>[[VS], 63323027205, 00641601310, 00069419068, ...</td>\n",
       "      <td>[[VS], 51009_4, 52301_3, 51564_0, 51044_3, [VE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[CLS], 7906, 7906, [VS], 0YHM0YZ, 100, 0YHM0Y...</td>\n",
       "      <td>[[VS], 68084061221, 51079017220, 00641601310, ...</td>\n",
       "      <td>[[VS], 52301_3, 50841_4, 50908_4, 51009_4, 510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[CLS], 7906, 0JC00ZZ, 7906, 100, [VS], 7906, ...</td>\n",
       "      <td>[[VS], 00078059620, [VE], [M_2], [REG], [VS], ...</td>\n",
       "      <td>[[VS], 52301_3, 51512_0, [VE], [M_2], [REG], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[CLS], [VS], 0B998ZZ, 02H64JZ, 0JC00ZZ, [VE],...</td>\n",
       "      <td>[[VS], 00078059620, 68084061221, 51079017220, ...</td>\n",
       "      <td>[[VS], 51044_3, 50841_4, 51512_0, 51429_0, 509...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[CLS], 9652, 9652, 0YHM0YZ, 0JC00ZZ, [VS], 79...</td>\n",
       "      <td>[[VS], 00078037742, 68084061221, 00007550040, ...</td>\n",
       "      <td>[[VS], 51429_0, 51429_0, 51512_0, 51044_3, [VE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[CLS], 100, [VS], 0JC00ZZ, 9652, [VE], [M_2],...</td>\n",
       "      <td>[[VS], 00641601310, [VE], [M_2], [REG], [VS], ...</td>\n",
       "      <td>[[VS], 50995_4, 50841_4, 51564_0, 51429_0, 508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[CLS], 8127, [VS], 7906, [VE], [W_1], [REG], ...</td>\n",
       "      <td>[[VS], 00069419068, 00078037742, 00078059620, ...</td>\n",
       "      <td>[[VS], 50908_4, 50908_4, 51009_4, 52301_3, 509...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           diagnosis  \\\n",
       "0  [[CLS], 02H64JZ, 8222, [VS], 100, 0JC00ZZ, 100...   \n",
       "1  [[CLS], 8222, 8127, [VS], 8127, [VE], [LT], [R...   \n",
       "2  [[CLS], 0JC00ZZ, 0YHM0YZ, [VS], 8222, 7906, [V...   \n",
       "3  [[CLS], 0JC00ZZ, 8127, [VS], 9652, 0JC00ZZ, 81...   \n",
       "4  [[CLS], 7906, 7906, [VS], 0YHM0YZ, 100, 0YHM0Y...   \n",
       "5  [[CLS], 7906, 0JC00ZZ, 7906, 100, [VS], 7906, ...   \n",
       "6  [[CLS], [VS], 0B998ZZ, 02H64JZ, 0JC00ZZ, [VE],...   \n",
       "7  [[CLS], 9652, 9652, 0YHM0YZ, 0JC00ZZ, [VS], 79...   \n",
       "8  [[CLS], 100, [VS], 0JC00ZZ, 9652, [VE], [M_2],...   \n",
       "9  [[CLS], 8127, [VS], 7906, [VE], [W_1], [REG], ...   \n",
       "\n",
       "                                           procedure  \\\n",
       "0  [[VS], 51079017220, 00069419068, 63323027205, ...   \n",
       "1  [[VS], 00078059620, 00641601310, [VE], [LT], [...   \n",
       "2  [[VS], 00078059620, 68084061221, [VE], [W_3], ...   \n",
       "3  [[VS], 63323027205, 00641601310, 00069419068, ...   \n",
       "4  [[VS], 68084061221, 51079017220, 00641601310, ...   \n",
       "5  [[VS], 00078059620, [VE], [M_2], [REG], [VS], ...   \n",
       "6  [[VS], 00078059620, 68084061221, 51079017220, ...   \n",
       "7  [[VS], 00078037742, 68084061221, 00007550040, ...   \n",
       "8  [[VS], 00641601310, [VE], [M_2], [REG], [VS], ...   \n",
       "9  [[VS], 00069419068, 00078037742, 00078059620, ...   \n",
       "\n",
       "                                                 lab  \n",
       "0  [[VS], 51044_3, 51429_0, 52301_3, 52178_0, 509...  \n",
       "1  [[VS], 51044_3, 50908_4, 50908_4, 50841_4, [VE...  \n",
       "2  [[VS], 51009_4, 51564_0, 50995_4, 50995_4, [VE...  \n",
       "3  [[VS], 51009_4, 52301_3, 51564_0, 51044_3, [VE...  \n",
       "4  [[VS], 52301_3, 50841_4, 50908_4, 51009_4, 510...  \n",
       "5  [[VS], 52301_3, 51512_0, [VE], [M_2], [REG], [...  \n",
       "6  [[VS], 51044_3, 50841_4, 51512_0, 51429_0, 509...  \n",
       "7  [[VS], 51429_0, 51429_0, 51512_0, 51044_3, [VE...  \n",
       "8  [[VS], 50995_4, 50841_4, 51564_0, 51429_0, 508...  \n",
       "9  [[VS], 50908_4, 50908_4, 51009_4, 52301_3, 509...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Define random dataset using the actual vocabulary ###\n",
    "lab_dict = [\"50995_4\", \"51429_0\", \"51009_4\", \"50908_4\", \"51564_0\", \"52301_3\", \"51044_3\", \"50841_4\", \"52178_0\", \"51512_0\"]\n",
    "procedure_dict = [\"00641601310\", \"00069419068\", \"00078037742\", \"63323027205\", \"00078059620\", \"00007550040\", \"51079017220\", \"68084061221\"]\n",
    "diagnosis_dict = [\"02H64JZ\", \"7906\", \"8222\", \"0JC00ZZ\", \"0YHM0YZ\", \"9652\", \"03743D6\", \"100\", \"0B998ZZ\", \"8127\"]\n",
    "time_dict = ['[W_1]', '[W_3]', '[M_2]', '[M_5]', '[LT]']\n",
    "\n",
    "\n",
    "def generate_random_events(num_visits, event_dict, time_tokens):\n",
    "    patient = []\n",
    "    length_visits = [randint(1, 6) for _ in range(num_visits)]\n",
    "\n",
    "    for i in range(num_visits):\n",
    "        patient.append('[VS]')\n",
    "        length_visit = length_visits[i]\n",
    "        random_events = [event_dict[randint(0, len(event_dict)-1)] for _ in range(length_visit)]\n",
    "        patient += random_events\n",
    "        patient.append('[VE]')\n",
    "\n",
    "        if i < num_visits - 1:\n",
    "            patient.append(time_tokens[i])\n",
    "\n",
    "        patient.append('[REG]')\n",
    "\n",
    "    return patient\n",
    "\n",
    "\n",
    "def generate_random_patient(lab_dict, procedure_dict, diagnosis_dict, time_dict):\n",
    "    num_visits = randint(1, 5)\n",
    "    time_tokens = [time_dict[randint(0, len(time_dict)-1)] for _ in range(num_visits)]\n",
    "\n",
    "    random_lab = generate_random_events(num_visits, lab_dict, time_tokens)\n",
    "    random_procedure = generate_random_events(num_visits, procedure_dict, time_tokens)\n",
    "    random_diagnosis = ['[CLS]'] + generate_random_events(num_visits, diagnosis_dict, time_tokens)\n",
    "\n",
    "    prior_vs_diagnosis = [diagnosis_dict[randint(0, len(diagnosis_dict)-1)] for _ in range(randint(0, 5))]\n",
    "    random_diagnosis = [random_diagnosis[0]] + prior_vs_diagnosis + random_diagnosis[1:]\n",
    "\n",
    "    return {\"diagnosis\":random_diagnosis, \"procedure\":random_procedure, \"lab\":random_lab}\n",
    "\n",
    "\n",
    "def generate_random_dataset(lab_dict, procedure_dict, diagnosis_dict, time_dict, num_patients=10):\n",
    "    patients = []\n",
    "\n",
    "    for i in range(num_patients):\n",
    "        patient = generate_random_patient(lab_dict, procedure_dict, diagnosis_dict, time_dict)\n",
    "        patients.append(patient)\n",
    "\n",
    "    return patients\n",
    "\n",
    "\n",
    "# Assume these are already truncated\n",
    "patients = generate_random_dataset(lab_dict, procedure_dict, diagnosis_dict, time_dict)\n",
    "patients = pd.DataFrame(patients)\n",
    "patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T21:52:51.197797200Z",
     "start_time": "2024-01-31T21:52:51.112528400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis:\n",
      "['[CLS]', '0JC00ZZ', '[VS]', '100', '03743D6', '9652', '0JC00ZZ', '0B998ZZ', '7906', '[VE]', '[M_5]', '[REG]', '[VS]', '0JC00ZZ', '8222', '[VE]', '[LT]', '[REG]', '[VS]', '100', '0YHM0YZ', '03743D6', '02H64JZ', '03743D6', '[VE]', '[M_5]', '[REG]', '[VS]', '0YHM0YZ', '7906', '0YHM0YZ', '[VE]', '[M_2]', '[REG]', '[VS]', '8222', '7906', '8127', '[VE]', '[REG]'] \n",
      "\n",
      "Old Procedure:\n",
      "['[VS]', '00641601310', '00007550040', '63323027205', '[VE]', '[M_5]', '[REG]', '[VS]', '00078037742', '[VE]', '[LT]', '[REG]', '[VS]', '63323027205', '68084061221', '00007550040', '63323027205', '00078037742', '00078037742', '[VE]', '[M_5]', '[REG]', '[VS]', '63323027205', '00069419068', '51079017220', '[VE]', '[M_2]', '[REG]', '[VS]', '00069419068', '63323027205', '00078037742', '63323027205', '51079017220', '63323027205', '[VE]', '[REG]'] \n",
      "\n",
      "New Procedure:\n",
      "[['[PAD]', '[PAD]', '[VS]', '00641601310', '00007550040', '63323027205', '[PAD]', '[PAD]', '[PAD]', '[VE]', '[M_5]', '[REG]', '[VS]', '00078037742', '[PAD]', '[VE]', '[LT]', '[REG]', '[VS]', '63323027205', '68084061221', '00007550040', '63323027205', '00078037742', '[VE]', '[M_5]', '[REG]', '[VS]', '63323027205', '00069419068', '51079017220', '[VE]', '[M_2]', '[REG]', '[VS]', '00069419068', '63323027205', '00078037742', '[VE]', '[REG]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[VS]', '00078037742', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[VE]', '[M_5]', '[REG]', '[VS]', '[PAD]', '[PAD]', '[PAD]', '[VE]', '[M_2]', '[REG]', '[VS]'], ['[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[VS]', '63323027205', '51079017220', '63323027205', '[VE]', '[REG]']]\n",
      "\n",
      "Len check: True\n"
     ]
    }
   ],
   "source": [
    "# Vertical Alignment For Single Example\n",
    "patient = patients.iloc[9]\n",
    "\n",
    "procedure_ref = []\n",
    "next_iter_procedure_ref = []\n",
    "new_procedure = [[]]\n",
    "\n",
    "d = 0; p = 0\n",
    "while patient['diagnosis'][d] != '[VS]':\n",
    "    new_procedure[-1].append('[PAD]')\n",
    "    d += 1\n",
    "\n",
    "\n",
    "while d < len(patient['diagnosis']):\n",
    "\n",
    "    if patient['procedure'][p] == '[VE]' and patient['diagnosis'][d] != '[VE]':\n",
    "        new_procedure[-1].append('[PAD]')\n",
    "        d += 1\n",
    "        continue\n",
    "\n",
    "    elif patient['procedure'][p] != '[VE]' and patient['diagnosis'][d] == '[VE]':\n",
    "        vs_index = len(new_procedure[-1]) - new_procedure[-1][::-1].index('[VS]') - 1\n",
    "        procedure_ref.append((p, vs_index))\n",
    "\n",
    "        while patient['procedure'][p] != '[VE]':\n",
    "            p += 1\n",
    "\n",
    "    new_procedure[-1].append(patient['procedure'][p])\n",
    "    d += 1; p += 1\n",
    "\n",
    "\n",
    "\n",
    "while procedure_ref or next_iter_procedure_ref:\n",
    "\n",
    "    if next_iter_procedure_ref:\n",
    "        procedure_ref = next_iter_procedure_ref.copy()\n",
    "        next_iter_procedure_ref = []\n",
    "\n",
    "    new_procedure.append([])\n",
    "\n",
    "    for i, (ref, vs_index) in enumerate(procedure_ref):\n",
    "\n",
    "        if len(new_procedure[-1]) == 0:\n",
    "            n = 0\n",
    "            while n <= vs_index:\n",
    "                current_token = new_procedure[0][n]\n",
    "                if current_token in special_tokens:\n",
    "                    new_procedure[-1].append(current_token)\n",
    "                else:\n",
    "                    new_procedure[-1].append('[PAD]')\n",
    "                n += 1\n",
    "\n",
    "        n = vs_index + 1\n",
    "        p = ref\n",
    "\n",
    "        while patient['procedure'][p] != '[VE]' and new_procedure[0][n] != '[VE]':\n",
    "            new_procedure[-1].append(patient['procedure'][p])\n",
    "            n += 1; p += 1\n",
    "\n",
    "        if patient['procedure'][p] != '[VE]' and new_procedure[0][n] == '[VE]':\n",
    "            next_iter_procedure_ref.append((p, vs_index))\n",
    "\n",
    "        procedure_ref.remove((ref, vs_index))\n",
    "\n",
    "        if len(procedure_ref) == 0:\n",
    "            next_vs_index = len(new_procedure[0]) - 1\n",
    "        else:\n",
    "            next_vs_index = procedure_ref[i][1]\n",
    "\n",
    "        while n <= next_vs_index:\n",
    "            current_token = new_procedure[0][n]\n",
    "            if current_token in special_tokens:\n",
    "                new_procedure[-1].append(current_token)\n",
    "            else:\n",
    "                new_procedure[-1].append('[PAD]')\n",
    "            n += 1\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Diagnosis:\\n{patient['diagnosis']} \\n\\nOld Procedure:\\n{patient['procedure']} \\n\\nNew Procedure:\\n{new_procedure}\\n\")\n",
    "print(f\"Len check: {len(new_procedure[0]) == len(patient['diagnosis'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:28:59.245346Z",
     "start_time": "2024-02-01T17:28:59.224348200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "\n",
    "vocab_json_files = glob.glob(os.path.join(config.data_dir, '*_vocab.json'))\n",
    "for file in vocab_json_files:\n",
    "    vocab = json.load(open(file, 'r'))\n",
    "\n",
    "    vocab_type = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    vocab_dict[vocab_type] = vocab\n",
    "\n",
    "combined_vocab = list(chain.from_iterable(list(vocab_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:28:59.300345700Z",
     "start_time": "2024-02-01T17:28:59.244351400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_vocab = special_tokens + combined_vocab\n",
    "tokenizer_vocab = {token: i for i, token in enumerate(combined_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:28:59.301345600Z",
     "start_time": "2024-02-01T17:28:59.248346Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK] [PAD] [CLS]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_object = Tokenizer(models.WordPiece(vocab=tokenizer_vocab, unk_token=\"[UNK]\", max_input_chars_per_word=1000))\n",
    "tokenizer_object.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "example = \" \".join(patients.iloc[5]['diagnosis'] + [\"[UNK] [PAD] [PAD] [PAD] [PAD]\"])\n",
    "example2 = \" \".join(patients.iloc[1]['lab'] + [\"[UNK] [PAD] [PAD]\"])\n",
    "encoding = tokenizer_object.decode([0, 1, 2])\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:31:52.259867300Z",
     "start_time": "2024-02-01T17:31:52.080395600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/afallah/light/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2637: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   2, 3064, 3066, 3064, 3070,    5, 3064, 3066, 3072,    6,   13,    3,\n",
       "            5, 3070, 3072, 3067, 3070,    6,   24,    3,    5, 3064, 3068, 3072,\n",
       "            6,    3,    0,    1,    1,    1,    1],\n",
       "        [   5,   31,   28,   28,   32,    6,   24,    3,    5,   34,    6,    3,\n",
       "            0,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer_object,\n",
    "        bos_token=\"[VS]\",\n",
    "        eos_token=\"[VE]\",\n",
    "        unk_token=\"[UNK]\",\n",
    "        # sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "    )\n",
    "\n",
    "tokenizer(\n",
    "    [example, example2],\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=True,\n",
    "    truncation=False,\n",
    "    padding=True,\n",
    "    max_length=2048,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T16:29:32.544261700Z",
     "start_time": "2024-01-30T16:29:32.511191200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConceptTokenizer:\n",
    "    \"\"\"Tokenizer for event concepts.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        pad_token: str = \"[PAD]\",\n",
    "        mask_token: str = \"[MASK]\",\n",
    "        start_token: str = \"[VS]\",\n",
    "        end_token: str = \"[VE]\",\n",
    "        oov_token='-1',\n",
    "        data_dir: str = 'data_files'\n",
    "    ):\n",
    "        self.tokenizer = Tokenizer(oov_token=oov_token, filters='', lower=False)\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "        self.special_tokens = [pad_token, mask_token, start_token, end_token] + \\\n",
    "            [f'W_{i}' for i in range(0, 4)] + [f'M_{i}' for i in range(0, 13)] + ['LT']\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def fit_on_vocab(self) -> None:\n",
    "        \"\"\"Fit the tokenizer on the vocabulary.\"\"\"\n",
    "        vocab_json_files = glob.glob(os.path.join(self.data_dir, '*_vocab.json'))\n",
    "        for file in vocab_json_files:\n",
    "            vocab = json.load(open(file, 'r'))\n",
    "            self.tokenizer.fit_on_texts(vocab)\n",
    "        self.tokenizer.fit_on_texts(self.special_tokens)\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        concept_sequences: Union[str, Sequence[str]],\n",
    "        is_generator: bool=False\n",
    "    ) -> Union[int, Sequence[int]]:\n",
    "        \"\"\"Encode the concept sequences into token ids.\"\"\"\n",
    "        return self.tokenizer.texts_to_sequences_generator(\n",
    "            concept_sequences) if is_generator else self.tokenizer.texts_to_sequences(\n",
    "            concept_sequences)\n",
    "\n",
    "    def decode(\n",
    "        self, concept_sequence_token_ids: Union[int, Sequence[int]]\n",
    "    ) -> Sequence[str]:\n",
    "        \"\"\"Decode the concept sequence token ids into concepts.\"\"\"\n",
    "        return self.tokenizer.sequences_to_texts(concept_sequence_token_ids)\n",
    "\n",
    "    def get_all_token_indexes(self) -> set:\n",
    "        all_keys = set(self.tokenizer.index_word.keys())\n",
    "\n",
    "        if self.tokenizer.oov_token is not None:\n",
    "            all_keys.remove(self.tokenizer.word_index[self.tokenizer.oov_token])\n",
    "\n",
    "        if self.special_tokens is not None:\n",
    "            excluded = set(\n",
    "                [self.tokenizer.word_index[special_token] for special_token in self.special_tokens])\n",
    "            all_keys = all_keys - excluded\n",
    "        return all_keys\n",
    "\n",
    "    def get_first_token_index(self) -> int:\n",
    "        return min(self.get_all_token_indexes())\n",
    "\n",
    "    def get_last_token_index(self) -> int:\n",
    "        return max(self.get_all_token_indexes())\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        # + 1 because oov_token takes the index 0\n",
    "        return len(self.tokenizer.index_word) + 1\n",
    "\n",
    "    def get_pad_token_id(self):\n",
    "        pad_token_id = self.encode(self.pad_token)\n",
    "        while isinstance(pad_token_id, list):\n",
    "            pad_token_id = pad_token_id[0]\n",
    "        return pad_token_id\n",
    "\n",
    "    def get_mask_token_id(self):\n",
    "        mask_token_id = self.encode(self.mask_token)\n",
    "        while isinstance(mask_token_id, list):\n",
    "            mask_token_id = mask_token_id[0]\n",
    "        return mask_token_id\n",
    "\n",
    "    def get_special_token_ids(self):\n",
    "        special_ids = self.encode(self.special_tokens)\n",
    "        flat_special_ids = [item[0] for item in special_ids]\n",
    "        return flat_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T16:29:35.531039100Z",
     "start_time": "2024-01-30T16:29:35.397097300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = ConceptTokenizer(data_dir=config.data_dir)\n",
    "tokenizer.fit_on_vocab()\n",
    "config.vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T16:29:37.760036100Z",
     "start_time": "2024-01-30T16:29:37.751038700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1 -1 50995_4 51429_0 51009_4 50908_4']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([[0, 1, 2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T16:29:44.709991800Z",
     "start_time": "2024-01-30T16:29:44.700973400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20569, 1, 1, 20571]]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode([\"[PAD] [UNKoieri] [CLS] [VS]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-29T16:01:10.216895200Z",
     "start_time": "2024-01-29T16:01:10.208895Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 3071, 3062, 3069, 3065, 3064, 3069, 5, 23, 4, 3070, 3066, 3063, 3068, 5],\n",
       " [4, 15727, 5, 23, 4, 15731, 15731, 15729, 15731, 15731, 5],\n",
       " [4, 32, 5, 23, 4, 31, 5]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(patients.iloc[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
