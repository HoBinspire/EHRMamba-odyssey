{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T15:14:24.841474400Z",
     "start_time": "2024-03-01T15:14:17.672003200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT = \"/fs01/home/afallah/odyssey/odyssey\"\n",
    "\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import os; os.chdir(ROOT)\n",
    "import pandas as pd\n",
    "\n",
    "from models.big_bird_cehr.data import PretrainDataset\n",
    "from models.big_bird_cehr.tokenizer import HuggingFaceConceptTokenizer\n",
    "\n",
    "\n",
    "DATA_ROOT = f\"{ROOT}/data/slurm_data/2048/one_month\"\n",
    "DATA_PATH = f\"{DATA_ROOT}/fine_test.parquet\"\n",
    "NEW_DATA_PATH = f\"{ROOT}/data/bigbird_data/patient_sequences_2048_labeled.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f5aced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T15:14:52.768051700Z",
     "start_time": "2024-03-01T15:14:24.841474400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"/h/afallah/odyssey/odyssey/data/bigbird_data/patient_sequences_2048_labeled.parquet\")\n",
    "patient_ids = pickle.load(open('/h/afallah/odyssey/odyssey/data/bigbird_data/dataset_2048_mortality_1month.pkl', 'rb'))\n",
    "pre_data = data.loc[data['patient_id'].isin(patient_ids['test'])]\n",
    "\n",
    "# Train Tokenizer\n",
    "tokenizer = HuggingFaceConceptTokenizer(data_dir=\"/h/afallah/odyssey/odyssey/data/vocab\")\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = PretrainDataset(\n",
    "    data=pre_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    "    mask_prob=0.15,\n",
    ")\n",
    "\n",
    "val_dataset = PretrainDataset(\n",
    "    data=pre_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    "    mask_prob=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0ceac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T15:14:52.803317400Z",
     "start_time": "2024-03-01T15:14:52.789656900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mask_tokens(self, sequence: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\" Mask the tokens in the sequence using vectorized operations.\"\"\"\n",
    "    mask_token_id = self.tokenizer.get_mask_token_id()\n",
    "\n",
    "    masked_sequence = sequence.clone()\n",
    "\n",
    "    # Ignore [PAD], [UNK], [MASK] tokens\n",
    "    prob_matrix = torch.full(masked_sequence.shape, self.mask_prob)\n",
    "    prob_matrix[torch.where(masked_sequence <= mask_token_id)] = 0\n",
    "    selected = torch.bernoulli(prob_matrix).bool()\n",
    "\n",
    "    # 80% of the time, replace masked input tokens with respective mask tokens\n",
    "    replaced = torch.bernoulli(torch.full(selected.shape, 0.8)).bool() & selected\n",
    "    masked_sequence[replaced] = mask_token_id\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random vector.\n",
    "    randomized = torch.bernoulli(torch.full(selected.shape, 0.1)).bool() & selected & ~replaced\n",
    "    random_idx = torch.randint(low=self.tokenizer.get_first_token_index(),\n",
    "                               high=self.tokenizer.get_last_token_index(),\n",
    "                               size=prob_matrix.shape, dtype=torch.long)\n",
    "    masked_sequence[randomized] = random_idx[randomized]\n",
    "\n",
    "    labels = torch.where(selected, sequence, -100)\n",
    "\n",
    "    return masked_sequence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a43549ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T15:21:55.944148400Z",
     "start_time": "2024-03-01T15:21:55.921896700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_dataset[0]['type_ids'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73756c75059737d3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-26T15:46:43.081570800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patients = pd.read_parquet(NEW_DATA_PATH)\n",
    "patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acdfaf82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T15:40:30.829556400Z",
     "start_time": "2024-02-26T15:40:30.745437200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = HuggingFaceConceptTokenizer(data_dir=DATA_ROOT)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "train_dataset = PretrainDataset(\n",
    "    data=patients,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    "    mask_prob=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eaf26ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T15:40:30.844276Z",
     "start_time": "2024-02-26T15:40:30.836556300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e1 = \"[CLS] [VS] 00054853516 00245008201 00338004904 00008084199 00045152510 00006003121\"\n",
    "e2 = \"[CLS] [VS] 00054853516 00245008201\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a5cfe30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T15:46:35.854499500Z",
     "start_time": "2024-02-26T15:46:35.589931400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    5,     0,     0,  ...,     0,     0,     0],\n",
       "        [    3,     0,     0,  ...,     0,     0,     0],\n",
       "        [12809,     0,     0,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 1352,     0,     0,  ...,     0,     0,     0],\n",
       "        [    4,     0,     0,  ...,     0,     0,     0],\n",
       "        [    6,     0,     0,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(patients[\"event_tokens_2048\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f181cd517e54378",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patients = patients[patients[\"event_tokens_2048\"].notnull()]\n",
    "\n",
    "tokenizer = ConceptTokenizer(data_dir=DATA_ROOT)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "train_dataset = PretrainDataset(\n",
    "    data=patients,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    "    mask_prob=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024b90f7a8241b4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.decode([[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ef0182b3a60b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patients.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147baf6cd3558cb5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a76df1a0358049",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_dataset[0][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65076f3cfce1754a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.get_mask_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34f08b3313b37b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list(train_dataset[110][\"concept_ids\"]).count(20569))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b9cef89b98c7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(train_dataset[110][\"concept_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765143ee1fbb703",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.get_pad_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb70c831ad4de96",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.encode([\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db816c5601357",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patients.iloc[0][\"event_tokens_2048\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af63bd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ROOT = \"/fs01/home/afallah/odyssey/odyssey\"\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.chdir(ROOT)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.big_bird_cehr.data import PretrainDataset\n",
    "from models.big_bird_cehr.tokenizer import ConceptTokenizer\n",
    "\n",
    "\n",
    "DATA_ROOT = f\"{ROOT}/data/slurm_data/2048/one_month\"\n",
    "DATA_PATH = f\"{DATA_ROOT}/fine_test.parquet\"\n",
    "patients = pd.read_parquet(DATA_PATH)\n",
    "patients\n",
    "# Find the unique set of all possible tokens, including special tokens\n",
    "unique_event_tokens = set()\n",
    "\n",
    "for patient_event_tokens in tqdm(\n",
    "        patients[\"event_tokens_2048\"].values, desc=\"Loading Tokens\", unit=\" Patients\",\n",
    "):\n",
    "    for event_token in patient_event_tokens:\n",
    "        unique_event_tokens.add(event_token)\n",
    "\n",
    "unique_event_tokens = list(unique_event_tokens)\n",
    "unique_event_tokens.sort(reverse=True)\n",
    "\n",
    "print(\n",
    "    f\"Complete list of unique event tokens\\nLength: {len(unique_event_tokens)}\\nHead: {unique_event_tokens[:30]}...\",\n",
    ")\n",
    "special_tokens = [\n",
    "    \"[CLS]\",\n",
    "    \"[PAD]\",\n",
    "    # \"[VS]\",\n",
    "    \"[VE]\",\n",
    "    \"[W_0]\",\n",
    "    \"[W_1]\",\n",
    "    \"[W_2]\",\n",
    "    \"[W_3]\",\n",
    "    *[f\"[M_{i}]\" for i in range(0, 13)],\n",
    "    \"[LT]\",\n",
    "]\n",
    "\n",
    "feature_event_tokens = [token for token in unique_event_tokens if token not in special_tokens]\n",
    "\n",
    "print(len(feature_event_tokens), feature_event_tokens[:20])\n",
    "patients_event_tokens = patients[\"event_tokens_2048\"]\n",
    "len_vocab = len(feature_event_tokens)\n",
    "token2id = {token: i for i, token in enumerate(feature_event_tokens)}\n",
    "token_correlations = np.zeros(shape=(len_vocab, len_vocab))\n",
    "token_frequencies = []\n",
    "\n",
    "for curr_token in tqdm(feature_event_tokens, desc=\"Analyzing... \", unit=\" Tokens\"):\n",
    "    curr_token_id = token2id[curr_token]\n",
    "    token_freq = 0\n",
    "\n",
    "    for _, patient in enumerate(patients_event_tokens):\n",
    "\n",
    "        vs_id = np.where(patient == \"[VS]\")[0]\n",
    "        ve_id = np.where(patient == \"[VE]\")[0]\n",
    "\n",
    "        for vs, ve in zip(vs_id, ve_id):\n",
    "            curr_visit = patient[vs:ve]\n",
    "\n",
    "            if curr_token not in curr_visit:\n",
    "                continue\n",
    "\n",
    "            token_freq += 1\n",
    "            for visit_token in curr_visit:\n",
    "                token_correlations[curr_token_id][token2id[visit_token]] += 1\n",
    "\n",
    "    token_frequencies.append(token_freq)\n",
    "patients = patients[patients[\"event_tokens_2048\"].notnull()]\n",
    "\n",
    "tokenizer = ConceptTokenizer(data_dir=DATA_ROOT)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "train_dataset = PretrainDataset(\n",
    "    data=patients,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    "    mask_prob=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
