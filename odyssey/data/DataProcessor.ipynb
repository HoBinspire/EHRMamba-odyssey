{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = \"/h/afallah/odyssey/odyssey\"\n",
    "DATA_ROOT = f\"{ROOT}/odyssey/data/bigbird_data\"\n",
    "DATASET = f\"{DATA_ROOT}/patient_sequences/patient_sequences_2048.parquet\"\n",
    "MAX_LEN = 2048\n",
    "\n",
    "os.chdir(ROOT)\n",
    "\n",
    "from odyssey.utils.utils import seed_everything\n",
    "from odyssey.data.tokenizer import ConceptTokenizer\n",
    "from odyssey.data.dataset import FinetuneMultiDataset\n",
    "from odyssey.data.processor import (\n",
    "    filter_by_num_visit,\n",
    "    filter_by_length_of_stay,\n",
    "    get_last_occurence_index,\n",
    "    check_readmission_label,\n",
    "    get_length_of_stay,\n",
    "    get_visit_cutoff_at_threshold,\n",
    "    process_length_of_stay_dataset,\n",
    "    process_condition_dataset,\n",
    "    process_mortality_dataset,\n",
    "    process_readmission_dataset,\n",
    "    process_multi_dataset,\n",
    "    stratified_train_test_split,\n",
    "    sample_balanced_subset,\n",
    "    get_pretrain_test_split,\n",
    "    get_finetune_split,\n",
    ")\n",
    "\n",
    "SEED = 23\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load complete dataset\n",
    "dataset_2048 = pd.read_parquet(DATASET)\n",
    "\n",
    "print(f\"Current columns: {dataset_2048.columns}\")\n",
    "dataset_2048.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset for length of stay prediction above a threshold\n",
    "dataset_2048_los = process_length_of_stay_dataset(\n",
    "    dataset_2048.copy(), threshold=7, max_len=MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset for conditions including rare and common\n",
    "dataset_2048_condition = process_condition_dataset(dataset_2048.copy(), max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset for mortality in two weeks or one month task\n",
    "dataset_2048_mortality = process_mortality_dataset(dataset_2048.copy(), max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset for hospital readmission in one month task\n",
    "dataset_2048_readmission = process_readmission_dataset(\n",
    "    dataset_2048.copy(), max_len=MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the multi dataset\n",
    "multi_dataset = process_multi_dataset(\n",
    "    datasets={\n",
    "        \"original\": dataset_2048,\n",
    "        \"mortality\": dataset_2048_mortality,\n",
    "        \"condition\": dataset_2048_condition,\n",
    "        \"readmission\": dataset_2048_readmission,\n",
    "        \"los\": dataset_2048_los,\n",
    "    },\n",
    "    max_len=MAX_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "patient_ids_dict = {\n",
    "    \"pretrain\": [],\n",
    "    \"finetune\": {\"few_shot\": {}, \"kfold\": {}},\n",
    "    \"test\": [],\n",
    "}\n",
    "\n",
    "# Get train-test split\n",
    "# pretrain_ids, test_ids = get_pretrain_test_split(dataset_2048_readmission, stratify_target='label_readmission_1month', test_size=0.2)\n",
    "# pretrain_ids, test_ids = get_pretrain_test_split(process_condition_dataset, stratify_target='all_conditions', test_size=0.15)\n",
    "# patient_ids_dict['pretrain'] = pretrain_ids\n",
    "# patient_ids_dict['test'] = test_ids\n",
    "\n",
    "# Load pretrain and test patient IDs\n",
    "pid = pickle.load(open(\"patient_id_dict/dataset_2048_multi.pkl\", \"rb\"))\n",
    "patient_ids_dict[\"pretrain\"] = pid[\"pretrain\"]\n",
    "patient_ids_dict[\"test\"] = pid[\"test\"]\n",
    "set(pid[\"pretrain\"] + pid[\"test\"]) == set(dataset_2048[\"patient_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_config = {\n",
    "    \"mortality\": {\n",
    "        \"dataset\": dataset_2048_mortality,\n",
    "        \"label_col\": \"label_mortality_1month\",\n",
    "        \"finetune_size\": [250, 500, 1000, 5000, 20000],\n",
    "        \"save_path\": \"patient_id_dict/dataset_2048_mortality.pkl\",\n",
    "        \"split_mode\": \"single_label_balanced\",\n",
    "    },\n",
    "    \"readmission\": {\n",
    "        \"dataset\": dataset_2048_readmission,\n",
    "        \"label_col\": \"label_readmission_1month\",\n",
    "        \"finetune_size\": [250, 1000, 5000, 20000, 60000],\n",
    "        \"save_path\": \"patient_id_dict/dataset_2048_readmission.pkl\",\n",
    "        \"split_mode\": \"single_label_stratified\",\n",
    "    },\n",
    "    \"length_of_stay\": {\n",
    "        \"dataset\": dataset_2048_los,\n",
    "        \"label_col\": \"label_los_1week\",\n",
    "        \"finetune_size\": [250, 1000, 5000, 20000, 50000],\n",
    "        \"save_path\": \"patient_id_dict/dataset_2048_los.pkl\",\n",
    "        \"split_mode\": \"single_label_balanced\",\n",
    "    },\n",
    "    \"condition\": {\n",
    "        \"dataset\": dataset_2048_condition,\n",
    "        \"label_col\": \"all_conditions\",\n",
    "        \"finetune_size\": [50000],\n",
    "        \"save_path\": \"patient_id_dict/dataset_2048_condition.pkl\",\n",
    "        \"split_mode\": \"multi_label_stratified\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get finetune split\n",
    "for task in task_config.keys():\n",
    "    patient_ids_dict = get_finetune_split(\n",
    "        task_config=task_config,\n",
    "        task=task,\n",
    "        patient_ids_dict=patient_ids_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2048_mortality.to_parquet(\n",
    "    \"patient_sequences/patient_sequences_2048_mortality.parquet\",\n",
    ")\n",
    "dataset_2048_readmission.to_parquet(\n",
    "    \"patient_sequences/patient_sequences_2048_readmission.parquet\",\n",
    ")\n",
    "dataset_2048_los.to_parquet(\"patient_sequences/patient_sequences_2048_los.parquet\")\n",
    "dataset_2048_condition.to_parquet(\n",
    "    \"patient_sequences/patient_sequences_2048_condition.parquet\",\n",
    ")\n",
    "multi_dataset.to_parquet(\"patient_sequences/patient_sequences_2048_multi.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# multi_dataset = pd.read_parquet('patient_sequences/patient_sequences_2048_multi.parquet')\n",
    "# pid = pickle.load(open('patient_id_dict/dataset_2048_multi.pkl', 'rb'))\n",
    "# multi_dataset = multi_dataset[multi_dataset['patient_id'].isin(pid['finetune']['few_shot']['all'])]\n",
    "\n",
    "# # Train Tokenizer\n",
    "# tokenizer = ConceptTokenizer(data_dir='/h/afallah/odyssey/odyssey/odyssey/data/vocab')\n",
    "# tokenizer.fit_on_vocab(with_tasks=True)\n",
    "\n",
    "# # Load datasets\n",
    "# tasks = ['mortality_1month', 'los_1week'] + [f'c{i}' for i in range(5)]\n",
    "\n",
    "# train_dataset = FinetuneMultiDataset(\n",
    "#     data=multi_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     tasks=tasks,\n",
    "#     balance_guide={'mortality_1month': 0.5, 'los_1week': 0.5},\n",
    "#     max_len=2048,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_2048_condition = pd.read_parquet('patient_sequences/patient_sequences_2048_condition.parquet')\n",
    "# pid = pickle.load(open('patient_id_dict/dataset_2048_condition.pkl', 'rb'))\n",
    "# condition_finetune = dataset_2048_condition.loc[dataset_2048_condition['patient_id'].isin(pid['finetune']['few_shot']['50000'])]\n",
    "# condition_finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq = np.array(condition_finetune['all_conditions'].tolist()).sum(axis=0)\n",
    "# weights = np.clip(0, 50, sum(freq) / freq)\n",
    "# np.max(np.sqrt(freq)) / np.sqrt(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(patient_ids_dict['pretrain']) == sorted(pickle.load(open('new_data/patient_id_dict/sample_pretrain_test_patient_ids_with_conditions.pkl', 'rb'))['pretrain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = pd.merge(dataset_2048_mortality, dataset_2048_readmission, how='outer', on='patient_id')\n",
    "# final_merged_df = pd.merge(merged_df, dataset_2048_condition, how='outer', on='patient_id')\n",
    "# final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing stratified k-fold split\n",
    "# skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# for i, (train_index, cv_index) in enumerate(skf.split(dataset, dataset[label_col])):\n",
    "\n",
    "#     dataset_cv = dataset.iloc[cv_index]\n",
    "#     dataset_finetune = dataset.iloc[train_index]\n",
    "\n",
    "#     # Separate positive and negative labeled patients\n",
    "#     pos_patients = dataset_cv[dataset_cv[label_col] == True]['patient_id'].tolist()\n",
    "#     neg_patients = dataset_cv[dataset_cv[label_col] == False]['patient_id'].tolist()\n",
    "\n",
    "#     # Calculate the number of positive and negative patients needed for balanced CV set\n",
    "#     num_pos_needed = cv_size // 2\n",
    "#     num_neg_needed = cv_size // 2\n",
    "\n",
    "#     # Select positive and negative patients for CV set ensuring balanced distribution\n",
    "#     cv_patients = pos_patients[:num_pos_needed] + neg_patients[:num_neg_needed]\n",
    "#     remaining_finetune_patients = pos_patients[num_pos_needed:] + neg_patients[num_neg_needed:]\n",
    "\n",
    "#     # Extract patient IDs for training set\n",
    "#     finetune_patients = dataset_finetune['patient_id'].tolist()\n",
    "#     finetune_patients += remaining_finetune_patients\n",
    "\n",
    "#     # Shuffle each list of patients\n",
    "#     random.shuffle(cv_patients)\n",
    "#     random.shuffle(finetune_patients)\n",
    "\n",
    "#     patient_ids_dict['finetune']['kfold'][f'group{i+1}'] = {'finetune': finetune_patients, 'cv': cv_patients}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset.event_tokens is your DataFrame column\n",
    "# dataset.event_tokens.transform(len).plot(kind='hist', bins=100)\n",
    "# plt.xlim(1000, 8000)  # Limit x-axis to 5000\n",
    "# plt.ylim(0, 6000)\n",
    "# plt.xlabel('Length of Event Tokens')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Histogram of Event Tokens Length')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(patient_ids_dict['group3']['cv'])\n",
    "\n",
    "# dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids_dict['group1']['cv'])]['label_mortality_1month']\n",
    "\n",
    "# s = set()\n",
    "# for i in range(1, 6):\n",
    "#     s = s.union(set(patient_ids_dict[f'group{i}']['cv']))\n",
    "#\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DEAD ZONE | DO NOT ENTER #####\n",
    "\n",
    "# patient_ids = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_1month.pkl'), 'rb'))\n",
    "# patient_ids['finetune']['few_shot'].keys()\n",
    "\n",
    "# patient_ids2 = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_2weeks.pkl'), 'rb'))['pretrain']\n",
    "#\n",
    "# patient_ids1.sort()\n",
    "# patient_ids2.sort()\n",
    "#\n",
    "# patient_ids1 == patient_ids2\n",
    "# # dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids['pretrain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_2048_readmission = dataset_2048.loc[dataset_2048['num_visits'] > 1]\n",
    "# dataset_2048_readmission.reset_index(drop=True, inplace=True)\n",
    "#\n",
    "# dataset_2048_readmission['last_VS_index'] = dataset_2048_readmission['event_tokens_2048'].transform(lambda seq: get_last_occurence_index(list(seq), '[VS]'))\n",
    "#\n",
    "# dataset_2048_readmission['label_readmission_1month'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][row['last_VS_index'] - 1] in ('[W_0]', '[W_1]', '[W_2]', '[W_3]', '[M_1]'), axis=1\n",
    "# )\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission.apply(\n",
    "#     lambda row: row['event_tokens_2048'][:row['last_VS_index'] - 1], axis=1\n",
    "# )\n",
    "# dataset_2048_readmission.drop(['deceased', 'death_after_start', 'death_after_end', 'length'], axis=1, inplace=True)\n",
    "# dataset_2048_readmission['num_visits'] -= 1\n",
    "# dataset_2048_readmission['token_length'] = dataset_2048_readmission['event_tokens_2048'].apply(len)\n",
    "# dataset_2048_readmission = dataset_2048_readmission.apply(lambda row: truncate_and_pad(row), axis=1)\n",
    "# dataset_2048_readmission['event_tokens_2048'] = dataset_2048_readmission['event_tokens_2048'].transform(\n",
    "#     lambda token_list: ' '.join(token_list)\n",
    "# )\n",
    "#\n",
    "# dataset_2048_readmission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
