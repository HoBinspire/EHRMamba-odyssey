{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from transformers import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = \"/fs01/home/afallah/odyssey/odyssey\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "from odyssey.data.tokenizer import ConceptTokenizer\n",
    "from odyssey.models.model_utils import load_finetune_data\n",
    "from odyssey.data.dataset import FinetuneMultiDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    \"\"\"Save the configuration arguments.\"\"\"\n",
    "\n",
    "    model_path = \"checkpoints/multibird_finetune/multibird_finetune/test_outputs/test_outputs_1ec842db.pt\"\n",
    "    vocab_dir = \"odyssey/data/vocab\"\n",
    "    data_dir = \"odyssey/data/bigbird_data\"\n",
    "    sequence_file = \"patient_sequences/patient_sequences_2048_multi.parquet\"\n",
    "    id_file = \"patient_id_dict/dataset_2048_multi.pkl\"\n",
    "    valid_scheme = \"few_shot\"\n",
    "    num_finetune_patients = \"all\"\n",
    "    # label_name = \"label_mortality_1month\"\n",
    "    tasks = ['mortality_1month', 'los_1week', 'c0', 'c1', 'c2']\n",
    "\n",
    "    max_len = 2048\n",
    "    batch_size = 1\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"\n",
    "    Calculate and return performance metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"Balanced Accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"AUROC\": roc_auc_score(y_true, y_prob),\n",
    "        \"Average Precision Score\": average_precision_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    metrics[\"AUC-PR\"] = auc(recall, precision)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Sequence file not found: odyssey/data/bigbird_data/patient_sequences/patient_sequences/patient_sequences_2048_multi.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_vocab()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m fine_tune, fine_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_finetune_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_scheme\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_finetune_patients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m FinetuneMultiDataset(\n\u001b[1;32m     15\u001b[0m             data\u001b[38;5;241m=\u001b[39mfine_test,\n\u001b[1;32m     16\u001b[0m             tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m             max_len\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_len,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# train_dataset = FinetuneMultiDataset(\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#             data=fine_tune,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#             tokenizer=tokenizer,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#             max_len=config.max_len,\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/afallah/odyssey/odyssey/odyssey/models/model_utils.py:110\u001b[0m, in \u001b[0;36mload_finetune_data\u001b[0;34m(data_dir, sequence_file, id_file, valid_scheme, num_finetune_patients)\u001b[0m\n\u001b[1;32m    107\u001b[0m id_path \u001b[38;5;241m=\u001b[39m join(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatient_id_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m, id_file)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(sequence_path):\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(id_path):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Sequence file not found: odyssey/data/bigbird_data/patient_sequences/patient_sequences/patient_sequences_2048_multi.parquet"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = ConceptTokenizer(data_dir=config.vocab_dir)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "# Load test data\n",
    "fine_tune, fine_test = load_finetune_data(\n",
    "    config.data_dir,\n",
    "    config.sequence_file,\n",
    "    config.id_file,\n",
    "    config.valid_scheme,\n",
    "    config.num_finetune_patients,\n",
    ")\n",
    "\n",
    "test_dataset = FinetuneMultiDataset(\n",
    "            data=fine_test,\n",
    "            tokenizer=tokenizer,\n",
    "            tasks=config.tasks,\n",
    "            balance_guide=None,\n",
    "            max_len=config.max_len,\n",
    ")\n",
    "\n",
    "# train_dataset = FinetuneMultiDataset(\n",
    "#             data=fine_tune,\n",
    "#             tokenizer=tokenizer,\n",
    "#             tasks=['los_1week'],\n",
    "#             balance_guide={'los_1week': 0.5},\n",
    "#             max_len=config.max_len,\n",
    "# )\n",
    "\n",
    "tasks = [test_dataset.index_mapper[i][1] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'logits'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outputs = torch.load(config.model_path, map_location=config.device)\n",
    "test_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = test_outputs['labels'].cpu().numpy()\n",
    "logits = test_outputs['logits'].cpu().numpy()\n",
    "probs = torch.sigmoid(torch.tensor(logits[:, 1])).cpu().numpy()\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Balanced Accuracy': 0.8043450887917278,\n",
       " 'F1 Score': 0.7121043864519712,\n",
       " 'Precision': 0.6445533358462119,\n",
       " 'Recall': 0.7954721662273221,\n",
       " 'AUROC': 0.8889347846976665,\n",
       " 'Average Precision Score': 0.5738031917760823,\n",
       " 'AUC-PR': 0.7505522277674918}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tasks we have are: tasks = ['mortality_1month', 'los_1week', 'c0', 'c1', 'c2']\n",
    "\n",
    "task_idx = []\n",
    "for i, task in enumerate(tasks):\n",
    "    if task == 'los_1week':\n",
    "        task_idx.append(i)\n",
    "\n",
    "\n",
    "calculate_metrics(labels[task_idx], preds[task_idx], probs[task_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Balanced Accuracy': 0.8220779574282759,\n",
       " 'F1 Score': 0.7388243277631691,\n",
       " 'Precision': 0.7633654688869412,\n",
       " 'Recall': 0.7158119658119658,\n",
       " 'AUROC': 0.9255784114401215,\n",
       " 'Average Precision Score': 0.6157970244149283,\n",
       " 'AUC-PR': 0.7742741610984505}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_test_outputs = torch.load('test_outputs.pt')\n",
    "target = 2\n",
    "\n",
    "labels = old_test_outputs[\"labels\"][:, target]\n",
    "logits = torch.sigmoid(old_test_outputs[\"logits\"][:, target])\n",
    "preds = (logits >= 0.5).int()\n",
    "calculate_metrics(labels, preds, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(34., dtype=torch.float64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('checkpoints/bigbird_finetune_with_condition/mortality_1month_20000_patients/best.ckpt')\n",
    "# model['state_dict']['model.bert.embeddings.word_embeddings.weight'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
