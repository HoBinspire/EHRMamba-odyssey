{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/fs01/home/afallah/odyssey/odyssey\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "os.chdir(ROOT)\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from odyssey.data.dataset import FinetuneDataset, PretrainDataset\n",
    "from odyssey.models.cehr_big_bird.model import BigBirdFinetune, BigBirdPretrain\n",
    "from odyssey.models.cehr_big_bird.tokenizer import HuggingFaceConceptTokenizer\n",
    "\n",
    "\n",
    "DATA_ROOT = f\"{ROOT}/data/slurm_data/2048/one_month\"\n",
    "DATA_PATH = f\"{DATA_ROOT}/fine_test.parquet\"\n",
    "NEW_DATA_PATH = f\"{ROOT}/data/bigbird_data/patient_sequences_2048_labeled.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\n",
    "    \"/h/afallah/odyssey/odyssey/data/bigbird_data/patient_sequences_2048_labeled.parquet\",\n",
    ")\n",
    "patient_ids = pickle.load(\n",
    "    open(\n",
    "        \"/h/afallah/odyssey/odyssey/data/bigbird_data/dataset_2048_mortality_1month.pkl\",\n",
    "        \"rb\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fine_tune = data.loc[\n",
    "    data[\"patient_id\"].isin(patient_ids[\"valid\"][\"few_shot\"][\"20000_patients\"])\n",
    "]\n",
    "fine_test = data.loc[data[\"patient_id\"].isin(patient_ids[\"test\"])]\n",
    "\n",
    "fine_tune.rename(columns={\"label_mortality_1month\": \"label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Tokenizer\n",
    "tokenizer = HuggingFaceConceptTokenizer(\n",
    "    data_dir=\"/h/afallah/odyssey/odyssey/data/vocab\",\n",
    ")\n",
    "tokenizer.fit_on_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FinetuneDataset(\n",
    "    data=fine_test,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=5,\n",
    "    # num_workers=3,\n",
    "    # persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = pd.DataFrame({\"batch_size\": [3], \"gpus\": [1], \"max_epochs\": [5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb326b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = BigBirdFinetune(\n",
    "    args=args,\n",
    "    pretrained_model=pretrained_model,\n",
    "    dataset_len=len(test_dataset),\n",
    ")\n",
    "finetuned_model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/h/afallah/odyssey/odyssey/checkpoints/bigbird_finetune/mortality_1month_20000_patients/best-v1.ckpt\",\n",
    "    )[\"state_dict\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1822499e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26bf064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "finetune_dataset = FinetuneDataset(\n",
    "    data=pre_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ec104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\n",
    "    \"/h/afallah/odyssey/odyssey/data/bigbird_data/patient_sequences_2048_labeled.parquet\",\n",
    ")\n",
    "patient_ids = pickle.load(\n",
    "    open(\n",
    "        \"/h/afallah/odyssey/odyssey/data/bigbird_data/dataset_2048_mortality_1month.pkl\",\n",
    "        \"rb\",\n",
    "    ),\n",
    ")\n",
    "pre_data = data.loc[data[\"patient_id\"].isin(patient_ids[\"test\"])]\n",
    "pre_data.rename(columns={\"label_mortality_1month\": \"label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ffd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    finetune_dataset,\n",
    "    batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205463c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = BigBirdPretrain(\n",
    "    args=args,\n",
    "    dataset_len=len(test_dataset),\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    padding_idx=tokenizer.get_pad_token_id(),\n",
    ")\n",
    "pretrained_model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/h/afallah/odyssey/odyssey/checkpoints/bigbird_pretraining_a100/best.ckpt\",\n",
    "    )[\"state_dict\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigBirdFinetune(\n",
    "    args=args,\n",
    "    dataset_len=len(finetune_dataset),\n",
    "    pretrained_model=pretrained_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedab2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "at = batch.pop(\"attention_mask\")\n",
    "labels = batch.pop(\"labels\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model(\n",
    "    inputs=tuple(batch.values()),\n",
    "    attention_mask=at,\n",
    "    labels=None,\n",
    ").logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inputs=tuple(batch.values()), attention_mask=at, labels=labels, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ceac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(self, sequence: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Mask the tokens in the sequence using vectorized operations.\"\"\"\n",
    "    mask_token_id = self.tokenizer.get_mask_token_id()\n",
    "\n",
    "    masked_sequence = sequence.clone()\n",
    "\n",
    "    # Ignore [PAD], [UNK], [MASK] tokens\n",
    "    prob_matrix = torch.full(masked_sequence.shape, self.mask_prob)\n",
    "    prob_matrix[torch.where(masked_sequence <= mask_token_id)] = 0\n",
    "    selected = torch.bernoulli(prob_matrix).bool()\n",
    "\n",
    "    # 80% of the time, replace masked input tokens with respective mask tokens\n",
    "    replaced = torch.bernoulli(torch.full(selected.shape, 0.8)).bool() & selected\n",
    "    masked_sequence[replaced] = mask_token_id\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random vector.\n",
    "    randomized = (\n",
    "        torch.bernoulli(torch.full(selected.shape, 0.1)).bool() & selected & ~replaced\n",
    "    )\n",
    "    random_idx = torch.randint(\n",
    "        low=self.tokenizer.get_first_token_index(),\n",
    "        high=self.tokenizer.get_last_token_index(),\n",
    "        size=prob_matrix.shape,\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "    masked_sequence[randomized] = random_idx[randomized]\n",
    "\n",
    "    labels = torch.where(selected, sequence, -100)\n",
    "\n",
    "    return masked_sequence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43549ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train_dataset[0][\"type_ids\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73756c75059737d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_parquet(NEW_DATA_PATH)\n",
    "patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfaf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = HuggingFaceConceptTokenizer(data_dir=DATA_ROOT)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "train_dataset = PretrainDataset(\n",
    "    data=patients,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    "    mask_prob=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = (\n",
    "    \"[CLS] [VS] 00054853516 00245008201 00338004904 00008084199 00045152510 00006003121\"\n",
    ")\n",
    "e2 = \"[CLS] [VS] 00054853516 00245008201\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5cfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(patients[\"event_tokens_2048\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f181cd517e54378",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = patients[patients[\"event_tokens_2048\"].notnull()]\n",
    "\n",
    "tokenizer = ConceptTokenizer(data_dir=DATA_ROOT)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "train_dataset = PretrainDataset(\n",
    "    data=patients,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    "    mask_prob=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024b90f7a8241b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ef0182b3a60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147baf6cd3558cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a76df1a0358049",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65076f3cfce1754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_mask_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34f08b3313b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(train_dataset[110][\"concept_ids\"]).count(20569))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b9cef89b98c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset[110][\"concept_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765143ee1fbb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_pad_token_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb70c831ad4de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode([\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db816c5601357",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.iloc[0][\"event_tokens_2048\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/fs01/home/afallah/odyssey/odyssey\"\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.chdir(ROOT)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from models.big_bird_cehr.data import PretrainDataset\n",
    "from models.big_bird_cehr.tokenizer import ConceptTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "DATA_ROOT = f\"{ROOT}/data/slurm_data/2048/one_month\"\n",
    "DATA_PATH = f\"{DATA_ROOT}/fine_test.parquet\"\n",
    "patients = pd.read_parquet(DATA_PATH)\n",
    "patients\n",
    "# Find the unique set of all possible tokens, including special tokens\n",
    "unique_event_tokens = set()\n",
    "\n",
    "for patient_event_tokens in tqdm(\n",
    "    patients[\"event_tokens_2048\"].values,\n",
    "    desc=\"Loading Tokens\",\n",
    "    unit=\" Patients\",\n",
    "):\n",
    "    for event_token in patient_event_tokens:\n",
    "        unique_event_tokens.add(event_token)\n",
    "\n",
    "unique_event_tokens = list(unique_event_tokens)\n",
    "unique_event_tokens.sort(reverse=True)\n",
    "\n",
    "print(\n",
    "    f\"Complete list of unique event tokens\\nLength: {len(unique_event_tokens)}\\nHead: {unique_event_tokens[:30]}...\",\n",
    ")\n",
    "special_tokens = [\n",
    "    \"[CLS]\",\n",
    "    \"[PAD]\",\n",
    "    # \"[VS]\",\n",
    "    \"[VE]\",\n",
    "    \"[W_0]\",\n",
    "    \"[W_1]\",\n",
    "    \"[W_2]\",\n",
    "    \"[W_3]\",\n",
    "    *[f\"[M_{i}]\" for i in range(0, 13)],\n",
    "    \"[LT]\",\n",
    "]\n",
    "\n",
    "feature_event_tokens = [\n",
    "    token for token in unique_event_tokens if token not in special_tokens\n",
    "]\n",
    "\n",
    "print(len(feature_event_tokens), feature_event_tokens[:20])\n",
    "patients_event_tokens = patients[\"event_tokens_2048\"]\n",
    "len_vocab = len(feature_event_tokens)\n",
    "token2id = {token: i for i, token in enumerate(feature_event_tokens)}\n",
    "token_correlations = np.zeros(shape=(len_vocab, len_vocab))\n",
    "token_frequencies = []\n",
    "\n",
    "for curr_token in tqdm(feature_event_tokens, desc=\"Analyzing... \", unit=\" Tokens\"):\n",
    "    curr_token_id = token2id[curr_token]\n",
    "    token_freq = 0\n",
    "\n",
    "    for _, patient in enumerate(patients_event_tokens):\n",
    "        vs_id = np.where(patient == \"[VS]\")[0]\n",
    "        ve_id = np.where(patient == \"[VE]\")[0]\n",
    "\n",
    "        for vs, ve in zip(vs_id, ve_id):\n",
    "            curr_visit = patient[vs:ve]\n",
    "\n",
    "            if curr_token not in curr_visit:\n",
    "                continue\n",
    "\n",
    "            token_freq += 1\n",
    "            for visit_token in curr_visit:\n",
    "                token_correlations[curr_token_id][token2id[visit_token]] += 1\n",
    "\n",
    "    token_frequencies.append(token_freq)\n",
    "patients = patients[patients[\"event_tokens_2048\"].notnull()]\n",
    "\n",
    "tokenizer = ConceptTokenizer(data_dir=DATA_ROOT)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "train_dataset = PretrainDataset(\n",
    "    data=patients,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=2048,\n",
    "    mask_prob=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
