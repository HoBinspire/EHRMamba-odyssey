{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "ROOT = \"/fs01/home/afallah/odyssey/odyssey\"\n",
    "os.chdir(ROOT)\n",
    "# from models.cehr_bert.tokenizer import ConceptTokenizer\n",
    "import glob\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_ROOT = f\"{ROOT}/data/slurm_data/2048/one_month\"\n",
    "DATA_PATH = f\"{DATA_ROOT}/pretrain.parquet\"\n",
    "TOKENIZER_PATH = f\"{DATA_ROOT}/tokenizer.json\"\n",
    "special_tokens = (\n",
    "    [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[MASK]\", \"[VS]\", \"[VE]\"]\n",
    "    + [f\"[W_{i}]\" for i in range(0, 4)]\n",
    "    + [f\"[M_{i}]\" for i in range(0, 13)]\n",
    "    + [\"[LT]\"]\n",
    ")\n",
    "\n",
    "# To be added [REG] token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfff5979c4709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    seed = 23\n",
    "    data_dir = DATA_ROOT\n",
    "    test_size = 0.2\n",
    "    batch_size = 64\n",
    "    num_workers = 2\n",
    "    vocab_size = None\n",
    "    embedding_size = 128\n",
    "    time_embeddings_size = 16\n",
    "    max_len = 512\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f188c4b86093192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "patients = pd.read_parquet(DATA_PATH)\n",
    "patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789c95079b33513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of all possible medical concepts\n",
    "vocab_dict = {}\n",
    "\n",
    "vocab_json_files = glob.glob(os.path.join(config.data_dir, \"*_vocab.json\"))\n",
    "for file in vocab_json_files:\n",
    "    vocab = json.load(open(file, \"r\"))\n",
    "\n",
    "    vocab_type = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    vocab_dict[vocab_type] = vocab\n",
    "\n",
    "combined_vocab = list(chain.from_iterable(list(vocab_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b48b3eeb19fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer dictionary\n",
    "combined_vocab = special_tokens + combined_vocab\n",
    "tokenizer_vocab = {token: i for i, token in enumerate(combined_vocab)}\n",
    "\n",
    "# Create the tokenizer object\n",
    "tokenizer_object = Tokenizer(\n",
    "    models.WordPiece(\n",
    "        vocab=tokenizer_vocab,\n",
    "        unk_token=\"[UNK]\",\n",
    "        max_input_chars_per_word=1000,\n",
    "    ),\n",
    ")\n",
    "tokenizer_object.pre_tokenizer = pre_tokenizers.WhitespaceSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277ab7dfe592f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Examples\n",
    "example = \" \".join(patients.iloc[5][\"diagnosis\"] + [\"[UNK] [PAD] [PAD] [PAD] [PAD]\"])\n",
    "example2 = \" \".join(patients.iloc[1][\"lab\"] + [\"[UNK] [PAD] [PAD]\"])\n",
    "encoding = tokenizer_object.decode([0, 1, 2])\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a005251c7b7b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer_object.save(path=TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77624eb92b2132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=TOKENIZER_PATH,\n",
    "    bos_token=\"[VS]\",\n",
    "    eos_token=\"[VE]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    # sep_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "tokenizer(\n",
    "    [example, example2],\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=2048,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
