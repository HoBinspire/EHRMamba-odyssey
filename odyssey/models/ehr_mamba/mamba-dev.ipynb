{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, List, Optional, Set, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers\n",
    "from transformers import (\n",
    "    BatchEncoding,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    MambaConfig,\n",
    "    MambaModel,\n",
    "    MambaForCausalLM,\n",
    "    MambaPreTrainedModel,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from torch import nn, optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.modeling_outputs import MaskedLMOutput, SequenceClassifierOutput\n",
    "\n",
    "# from mamba_ssm.models.mixer_seq_simple import MambaConfig, MambaLMHeadModel\n",
    "\n",
    "ROOT = \"/h/afallah/odyssey/odyssey\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "from odyssey.models.embeddings import *\n",
    "from odyssey.data.dataset import (\n",
    "    PretrainDataset,\n",
    "    PretrainDatasetDecoder,\n",
    "    FinetuneDatasetDecoder,\n",
    ")\n",
    "from odyssey.data.tokenizer import ConceptTokenizer\n",
    "from odyssey.models.cehr_bert.model import BertPretrain\n",
    "from odyssey.models.cehr_big_bird.model import BigBirdPretrain\n",
    "from odyssey.models.model_utils import (\n",
    "    get_run_id,\n",
    "    load_config,\n",
    "    load_pretrain_data,\n",
    "    load_finetune_data,\n",
    ")\n",
    "from odyssey.utils.utils import seed_everything\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "New Training:\n",
    "1. Num parameters\n",
    "2. Epochs\n",
    "3. Overfitting\n",
    "    4. Emebeddings\n",
    "    5. Label balance\n",
    "    6. Dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    vocab_dir = \"odyssey/data/vocab\"\n",
    "    data_dir = \"odyssey/data/bigbird_data\"\n",
    "    sequence_file = \"patient_sequences_2048_multi_v2.parquet\"\n",
    "    id_file = \"dataset_2048_multi_v2.pkl\"\n",
    "    valid_scheme = \"few_shot\"\n",
    "    num_finetune_patients = \"all\"\n",
    "    max_len = 2048\n",
    "    mask_prob = 0.15\n",
    "    tasks = [\"mortality_1month\", \"los_1week\", \"c0\", \"c1\", \"c2\"]\n",
    "    balance_guide = {\n",
    "        \"mortality_1month\": 0.5,\n",
    "        \"los_1week\": 0.5,\n",
    "        \"c0\": 0.5,\n",
    "        \"c1\": 0.5,\n",
    "        \"c2\": 0.5,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tokenizer\n",
    "tokenizer = ConceptTokenizer(data_dir=config.vocab_dir)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "# Setup data\n",
    "# pre_data = load_pretrain_data(\n",
    "#     args.data_dir,\n",
    "#     f\"patient_sequences/{args.sequence_file}\",\n",
    "#     f\"patient_id_dict/{args.id_file}\",\n",
    "# )\n",
    "# train_dataset = PretrainDatasetDecoder(\n",
    "#     data=pre_data,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_len=args.max_len,\n",
    "# )\n",
    "\n",
    "# fine_train, fine_test = load_finetune_data(\n",
    "#     args.data_dir, args.sequence_file, args.id_file, \"few_shot\", \"all\"\n",
    "# )\n",
    "# test_dataset = PretrainDatasetDecoder(\n",
    "#     data=fine_test,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_len=args.max_len,\n",
    "# )\n",
    "\n",
    "# Load test data\n",
    "fine_tune, fine_test = load_finetune_data(\n",
    "    config.data_dir,\n",
    "    config.sequence_file,\n",
    "    config.id_file,\n",
    "    config.valid_scheme,\n",
    "    config.num_finetune_patients,\n",
    ")\n",
    "test_dataset = FinetuneDatasetDecoder(\n",
    "    data=fine_test,\n",
    "    tokenizer=tokenizer,\n",
    "    tasks=config.tasks,\n",
    "    balance_guide=None,\n",
    "    max_len=config.max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concept_ids': tensor([[    5,     3, 17362,  ...,     0,     0,     0],\n",
       "         [    5,     3,  1567,  ...,     0,     0,     0],\n",
       "         [    5,     3, 19482,  ...,     0,     0,     0]]),\n",
       " 'type_ids': tensor([[1, 2, 6,  ..., 0, 0, 0],\n",
       "         [1, 2, 5,  ..., 0, 0, 0],\n",
       "         [1, 2, 6,  ..., 0, 0, 0]]),\n",
       " 'ages': tensor([[ 0, 85, 85,  ...,  0,  0,  0],\n",
       "         [ 0, 83, 83,  ...,  0,  0,  0],\n",
       "         [ 0, 72, 72,  ...,  0,  0,  0]]),\n",
       " 'time_stamps': tensor([[   0, 5902, 5902,  ...,    0,    0,    0],\n",
       "         [   0, 6594, 6594,  ...,    0,    0,    0],\n",
       "         [   0, 6093, 6093,  ...,    0,    0,    0]]),\n",
       " 'visit_orders': tensor([[0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'visit_segments': tensor([[0, 2, 2,  ..., 0, 0, 0],\n",
       "         [0, 2, 2,  ..., 0, 0, 0],\n",
       "         [0, 2, 2,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([0, 0, 0]),\n",
       " 'task': ['mortality_1month', 'mortality_1month', 'mortality_1month'],\n",
       " 'task_idx': tensor([0, 0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(DataLoader(test_dataset, batch_size=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MambaConfig(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    hidden_size=768,\n",
    "    state_size=16,\n",
    "    num_hidden_layers=32,\n",
    "    max_seq_length=2048,\n",
    "    pad_token_id=tokenizer.get_pad_token_id(),\n",
    "    bos_token_id=tokenizer.token_to_id(\"[CLS]\"),\n",
    "    eos_token_id=tokenizer.get_pad_token_id(),\n",
    ")\n",
    "\n",
    "# embeddings = MambaEmbeddingsForCEHR(\n",
    "#     config=config\n",
    "# )\n",
    "\n",
    "model = MambaForCausalLM(config=config)\n",
    "# model.backbone.embeddings = embeddings\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "checkpoint = torch.load(\"checkpoints/mamba_pretrain/best.ckpt\", map_location=device)\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(state_dict)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    decoder_dataset,  # test_dataset, #train_dataset\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "sample = decoder_dataset[2323]  # test_dataset[8765] #train_dataset[0]\n",
    "task = sample.pop(\"task\")\n",
    "sample = {key: tensor.unsqueeze(0).to(device) for key, tensor in sample.items()}\n",
    "sample[\"task\"] = task\n",
    "\n",
    "# sample = next(iter(train_loader))\n",
    "# sample = {key:tensor.to(device) for key, tensor in sample.items()}\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = sample[\"concept_ids\"].squeeze().tolist()\n",
    "input_ids = input_ids[: input_ids.index(0)]\n",
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    torch.tensor(input_ids[:-10], dtype=torch.int32).unsqueeze(0).to(device),\n",
    "    max_new_tokens=10,\n",
    ")\n",
    "\n",
    "tokenizer.decode(output.squeeze().tolist()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, MambaForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "# model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "\n",
    "# inputs = tokenizer([\"Hello, my dog is cute\", \"NO\", \"Go to Sumeru\"], padding=True, return_tensors=\"pt\")\n",
    "# outputs = model(inputs['input_ids'], labels=inputs[\"input_ids\"])\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "# model.backbone.embeddings.cache_input(\n",
    "#     token_type_ids_batch = sample['type_ids'],\n",
    "#     position_ids_batch = None,\n",
    "#     inputs_embeds = None,\n",
    "#     time_stamps = sample['time_stamps'],\n",
    "#     ages = sample['ages'],\n",
    "#     visit_orders = sample['visit_orders'],\n",
    "#     visit_segments = sample['visit_segments']\n",
    "# )\n",
    "\n",
    "# outputs = model(\n",
    "#     input_ids=sample[\"concept_ids\"], labels=sample[\"concept_ids\"], return_dict=True\n",
    "# )\n",
    "\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.backbone\n",
    "outputs = model(input_ids=sample[\"concept_ids\"], return_dict=True)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.hidden_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = torch.nn.Linear(config.hidden_size, 2, bias=False).to(device)\n",
    "logits = classifier(last_hidden_states)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"concept_ids\"].squeeze()[204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = torch.eq(sample[\"concept_ids\"], 0).int().argmax(-1) - 1\n",
    "sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_logits = logits[torch.arange(1, device=device), sequence_lengths]\n",
    "pooled_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_last_hidden_states = last_hidden_states[\n",
    "    torch.arange(1, device=device), sequence_lengths\n",
    "]\n",
    "classifier(pooled_last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "config_copy = copy.deepcopy(config)\n",
    "config_copy.classifier_dropout = 0.1\n",
    "head = MambaClassificationHead(config_copy).to(device)\n",
    "head(pooled_last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fct(pooled_logits.view(-1, 2), torch.tensor([0]).to(device).view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odyssey.models.cehr_mamba.model import MambaPretrain\n",
    "from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Emebeddings -> Not now\n",
    "2. Padding order -> Done automatically\n",
    "\n",
    "---\n",
    "Finetuning Approach:\n",
    "    1. Replace the first and last REG token with the class token\n",
    "2. Use the last hiddent state of the last token for class prediction\n",
    "3. Ourselves!\n",
    "\n",
    "4. Dataset refactoring (inheritance, what to return, etc)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from odyssey.data.tokenizer import ConceptTokenizer, truncate_and_pad\n",
    "\n",
    "TASK_INDEX = 1\n",
    "LABEL_INDEX = 2\n",
    "CUTOFF_INDEX = 3\n",
    "\n",
    "\n",
    "# Load FinetuneDatasetDecoder for debugging\n",
    "\n",
    "\n",
    "decoder_dataset = FinetuneDatasetDecoder(\n",
    "    data=fine_test,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=args.max_len,\n",
    "    tasks=args.tasks,\n",
    "    balance_guide=args.balance_guide,\n",
    ")\n",
    "decoder_dataset[12112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odyssey.models.embeddings import *\n",
    "\n",
    "\n",
    "embeddings = MambaEmbeddingsForCEHR(\n",
    "    config=config,\n",
    "    type_vocab_size=9,\n",
    "    max_num_visits=512,\n",
    "    time_embeddings_size=32,\n",
    "    visit_order_size=3,\n",
    "    hidden_dropout_prob=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_dataset[51020]\n",
    "batch = {key: tensor.unsqueeze(0) for key, tensor in batch.items()}\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(batch[\"visit_orders\"][0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = (\n",
    "    batch[\"concept_ids\"],\n",
    "    batch[\"type_ids\"],\n",
    "    batch[\"time_stamps\"],\n",
    "    batch[\"ages\"],\n",
    "    batch[\"visit_orders\"],\n",
    "    batch[\"visit_segments\"],\n",
    ")\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "concept_ids, type_ids, time_stamps, ages, visit_orders, visit_segments = inputs\n",
    "inputs_embeds = embeddings(\n",
    "    input_ids=concept_ids,\n",
    "    token_type_ids_batch=type_ids,\n",
    "    time_stamps=time_stamps,\n",
    "    ages=ages,\n",
    "    visit_orders=visit_orders,\n",
    "    visit_segments=visit_segments,\n",
    ")\n",
    "inputs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(\n",
    "    inputs_embeds=inputs_embeds.to(device),\n",
    "    labels=labels.to(device),\n",
    "    output_hidden_states=False,\n",
    "    return_dict=True,\n",
    ")\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
