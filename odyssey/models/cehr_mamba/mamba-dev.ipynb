{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNew Training:\\n1. Num parameters\\n2. Epochs\\n3. Overfitting\\n    4. Emebeddings\\n    5. Label balance\\n    6. Dataset\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, List, Optional, Set, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers\n",
    "from transformers import (\n",
    "    BatchEncoding,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    MambaConfig,\n",
    "    MambaModel,\n",
    "    MambaForCausalLM,\n",
    "    MambaPreTrainedModel,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from torch import nn, optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.modeling_outputs import MaskedLMOutput, SequenceClassifierOutput\n",
    "\n",
    "# from mamba_ssm.models.mixer_seq_simple import MambaConfig, MambaLMHeadModel\n",
    "\n",
    "ROOT = \"/h/afallah/odyssey/odyssey\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "from odyssey.models.embeddings import *\n",
    "from odyssey.data.dataset import PretrainDataset, PretrainDatasetDecoder\n",
    "from odyssey.data.tokenizer import ConceptTokenizer\n",
    "from odyssey.models.cehr_bert.model import BertPretrain\n",
    "from odyssey.models.cehr_big_bird.model import BigBirdPretrain\n",
    "from odyssey.models.model_utils import (\n",
    "    get_run_id,\n",
    "    load_config,\n",
    "    load_pretrain_data,\n",
    "    load_finetune_data,\n",
    ")\n",
    "from odyssey.utils.utils import seed_everything\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "New Training:\n",
    "1. Num parameters\n",
    "2. Epochs\n",
    "3. Overfitting\n",
    "    4. Emebeddings\n",
    "    5. Label balance\n",
    "    6. Dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"checkpoints/mamba_pretrain_with_embeddings/best.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    data_dir = \"odyssey/data/bigbird_data\"\n",
    "    sequence_file = \"patient_sequences_2048_multi.parquet\"\n",
    "    id_file = \"dataset_2048_multi.pkl\"\n",
    "    vocab_dir = \"odyssey/data/vocab\"\n",
    "    max_len = 2048\n",
    "    mask_prob = 0.15\n",
    "    tasks = [\"mortality_1month\", \"los_1week\", \"c0\", \"c1\", \"c2\"]\n",
    "    balance_guide = {\n",
    "        \"mortality_1month\": 0.5,\n",
    "        \"los_1week\": 0.5,\n",
    "        \"c0\": 0.5,\n",
    "        \"c1\": 0.5,\n",
    "        \"c2\": 0.5,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tokenizer\n",
    "tokenizer = ConceptTokenizer(data_dir=args.vocab_dir)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "\n",
    "# Setup data\n",
    "pre_data = load_pretrain_data(\n",
    "    args.data_dir,\n",
    "    f\"patient_sequences/{args.sequence_file}\",\n",
    "    f\"patient_id_dict/{args.id_file}\",\n",
    ")\n",
    "train_dataset = PretrainDatasetDecoder(\n",
    "    data=pre_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=args.max_len,\n",
    ")\n",
    "\n",
    "\n",
    "# _, fine_test = load_finetune_data(\n",
    "#     args.data_dir, args.sequence_file, args.id_file, \"few_shot\", \"all\"\n",
    "# )\n",
    "# test_dataset = PretrainDatasetDecoder(\n",
    "#     data=fine_test,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_len=args.max_len,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(20600, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=20600, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = MambaConfig(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    hidden_size=768,\n",
    "    state_size=16,\n",
    "    num_hidden_layers=32,\n",
    "    max_seq_length=2048,\n",
    "    pad_token_id=tokenizer.get_pad_token_id(),\n",
    "    bos_token_id=tokenizer.token_to_id(\"[CLS]\"),\n",
    "    eos_token_id=tokenizer.get_pad_token_id(),\n",
    ")\n",
    "\n",
    "# embeddings = MambaEmbeddingsForCEHR(\n",
    "#     config=config\n",
    "# )\n",
    "\n",
    "model = MambaForCausalLM(config=config)\n",
    "# model.backbone.embeddings = embeddings\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "checkpoint = torch.load(\"checkpoints/mamba_pretrain/best.ckpt\", map_location=device)\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(state_dict)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    decoder_dataset,  # test_dataset, #train_dataset\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "sample = decoder_dataset[2323]  # test_dataset[8765] #train_dataset[0]\n",
    "task = sample.pop(\"task\")\n",
    "sample = {key: tensor.unsqueeze(0).to(device) for key, tensor in sample.items()}\n",
    "sample[\"task\"] = task\n",
    "\n",
    "# sample = next(iter(train_loader))\n",
    "# sample = {key:tensor.to(device) for key, tensor in sample.items()}\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = sample[\"concept_ids\"].squeeze().tolist()\n",
    "input_ids = input_ids[: input_ids.index(0)]\n",
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(\n",
    "    torch.tensor(input_ids[:-10], dtype=torch.int32).unsqueeze(0).to(device),\n",
    "    max_new_tokens=10,\n",
    ")\n",
    "\n",
    "tokenizer.decode(output.squeeze().tolist()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, MambaForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "# model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "\n",
    "# inputs = tokenizer([\"Hello, my dog is cute\", \"NO\", \"Go to Sumeru\"], padding=True, return_tensors=\"pt\")\n",
    "# outputs = model(inputs['input_ids'], labels=inputs[\"input_ids\"])\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "# model.backbone.embeddings.cache_input(\n",
    "#     token_type_ids_batch = sample['type_ids'],\n",
    "#     position_ids_batch = None,\n",
    "#     inputs_embeds = None,\n",
    "#     time_stamps = sample['time_stamps'],\n",
    "#     ages = sample['ages'],\n",
    "#     visit_orders = sample['visit_orders'],\n",
    "#     visit_segments = sample['visit_segments']\n",
    "# )\n",
    "\n",
    "# outputs = model(\n",
    "#     input_ids=sample[\"concept_ids\"], labels=sample[\"concept_ids\"], return_dict=True\n",
    "# )\n",
    "\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.backbone\n",
    "outputs = model(input_ids=sample[\"concept_ids\"], return_dict=True)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.hidden_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = torch.nn.Linear(config.hidden_size, 2, bias=False).to(device)\n",
    "logits = classifier(last_hidden_states)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[\"concept_ids\"].squeeze()[204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = torch.eq(sample[\"concept_ids\"], 0).int().argmax(-1) - 1\n",
    "sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_logits = logits[torch.arange(1, device=device), sequence_lengths]\n",
    "pooled_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_last_hidden_states = last_hidden_states[\n",
    "    torch.arange(1, device=device), sequence_lengths\n",
    "]\n",
    "classifier(pooled_last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "config_copy = copy.deepcopy(config)\n",
    "config_copy.classifier_dropout = 0.1\n",
    "head = MambaClassificationHead(config_copy).to(device)\n",
    "head(pooled_last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fct(pooled_logits.view(-1, 2), torch.tensor([0]).to(device).view(-1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states[:, 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odyssey.models.cehr_mamba.model import MambaPretrain\n",
    "from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Emebeddings -> Not now\n",
    "2. Padding order -> Done automatically\n",
    "\n",
    "---\n",
    "Finetuning Approach:\n",
    "    1. Replace the first and last REG token with the class token\n",
    "2. Use the last hiddent state of the last token for class prediction\n",
    "3. Ourselves!\n",
    "\n",
    "4. Dataset refactoring (inheritance, what to return, etc)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from odyssey.data.tokenizer import ConceptTokenizer, truncate_and_pad\n",
    "\n",
    "TASK_INDEX = 1\n",
    "LABEL_INDEX = 2\n",
    "CUTOFF_INDEX = 3\n",
    "\n",
    "\n",
    "# Load FinetuneDatasetDecoder for debugging\n",
    "\n",
    "\n",
    "decoder_dataset = FinetuneDatasetDecoder(\n",
    "    data=fine_test,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=args.max_len,\n",
    "    tasks=args.tasks,\n",
    "    balance_guide=args.balance_guide,\n",
    ")\n",
    "decoder_dataset[12112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odyssey.models.embeddings import *\n",
    "\n",
    "\n",
    "embeddings = MambaEmbeddingsForCEHR(\n",
    "    config=config,\n",
    "    type_vocab_size=9,\n",
    "    max_num_visits=512,\n",
    "    time_embeddings_size=32,\n",
    "    visit_order_size=3,\n",
    "    hidden_dropout_prob=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concept_ids': tensor([[    5,     3, 18896,  ...,  1712,     4,     6]]),\n",
       " 'type_ids': tensor([[1, 2, 6,  ..., 5, 3, 8]]),\n",
       " 'ages': tensor([[ 0, 77, 77,  ..., 78, 78, 78]]),\n",
       " 'time_stamps': tensor([[   0, 8928, 8928,  ..., 8981, 8981, 8981]]),\n",
       " 'visit_orders': tensor([[0, 1, 1,  ..., 8, 8, 8]]),\n",
       " 'visit_segments': tensor([[0, 2, 2,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([[    5,     3, 18896,  ...,  1712,     4,     6]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = train_dataset[51020]\n",
    "batch = {key: tensor.unsqueeze(0) for key, tensor in batch.items()}\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "print(set(batch[\"visit_orders\"][0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0452, -0.9307,  0.3723,  ..., -1.1524,  0.5854, -0.3397],\n",
       "         [ 0.0355, -0.5227, -3.0730,  ...,  0.0355, -0.9060, -0.9247],\n",
       "         [ 0.5338, -0.9962, -1.5450,  ...,  1.6476, -1.0616, -1.5152],\n",
       "         ...,\n",
       "         [ 0.1515,  1.6252, -0.2081,  ..., -1.0206,  0.8621,  1.3194],\n",
       "         [-0.0812,  2.3611, -0.1516,  ..., -1.0140, -0.0978,  1.6653],\n",
       "         [ 0.4165,  0.8611, -0.5180,  ...,  0.3821,  1.1638,  1.4207]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = (\n",
    "    batch[\"concept_ids\"],\n",
    "    batch[\"type_ids\"],\n",
    "    batch[\"time_stamps\"],\n",
    "    batch[\"ages\"],\n",
    "    batch[\"visit_orders\"],\n",
    "    batch[\"visit_segments\"],\n",
    ")\n",
    "labels = batch[\"labels\"]\n",
    "\n",
    "concept_ids, type_ids, time_stamps, ages, visit_orders, visit_segments = inputs\n",
    "inputs_embeds = embeddings(\n",
    "    input_ids=concept_ids,\n",
    "    token_type_ids_batch=type_ids,\n",
    "    time_stamps=time_stamps,\n",
    "    ages=ages,\n",
    "    visit_orders=visit_orders,\n",
    "    visit_segments=visit_segments,\n",
    ")\n",
    "inputs_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaCausalLMOutput(loss=tensor(14.1312, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[-1.7062, -5.0347,  2.0794,  ...,  2.3091,  1.0115, -1.9545],\n",
       "         [-1.0205, -2.8787, -4.8018,  ..., -3.2100, -5.3467, -1.1486],\n",
       "         [ 1.2367, -4.0578, -3.7514,  ..., -0.0644,  0.9085,  0.9692],\n",
       "         ...,\n",
       "         [ 2.3067,  3.5723,  1.9051,  ..., -0.0123, -2.5649, -0.4133],\n",
       "         [ 4.0669,  4.1643,  3.6506,  ...,  2.8866, -5.4374, -0.8073],\n",
       "         [ 1.8762,  5.5222, -0.6316,  ...,  0.1687, -7.1170, -4.6202]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), cache_params=None, hidden_states=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(\n",
    "    inputs_embeds=inputs_embeds.to(device),\n",
    "    labels=labels.to(device),\n",
    "    output_hidden_states=False,\n",
    "    return_dict=True,\n",
    ")\n",
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
