{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from itertools import chain\n",
    "from typing import Any, Dict, List, Optional, Set, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers\n",
    "from transformers import (\n",
    "    BatchEncoding,\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoTokenizer,\n",
    "    MambaConfig,\n",
    "    MambaModel,\n",
    "    MambaForCausalLM,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from torch import nn, optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.modeling_outputs import MaskedLMOutput, SequenceClassifierOutput\n",
    "\n",
    "# from mamba_ssm.models.mixer_seq_simple import MambaConfig, MambaLMHeadModel\n",
    "\n",
    "ROOT = \"/h/afallah/odyssey/odyssey\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "from odyssey.models.embeddings import *\n",
    "from odyssey.data.dataset import PretrainDataset, PretrainDatasetDecoder\n",
    "from odyssey.data.tokenizer import ConceptTokenizer\n",
    "from odyssey.models.cehr_bert.model import BertPretrain\n",
    "from odyssey.models.cehr_big_bird.model import BigBirdPretrain\n",
    "from odyssey.models.model_utils import (\n",
    "    get_run_id,\n",
    "    load_config,\n",
    "    load_pretrain_data,\n",
    "    load_finetune_data,\n",
    ")\n",
    "from odyssey.utils.utils import seed_everything\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    data_dir = \"odyssey/data/bigbird_data\"\n",
    "    sequence_file = \"patient_sequences_2048.parquet\"\n",
    "    id_file = \"dataset_2048_multi.pkl\"\n",
    "    vocab_dir = \"odyssey/data/vocab\"\n",
    "    max_len = 2048\n",
    "    mask_prob = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tokenizer\n",
    "tokenizer = ConceptTokenizer(data_dir=args.vocab_dir)\n",
    "tokenizer.fit_on_vocab()\n",
    "\n",
    "\n",
    "# Setup data\n",
    "# pre_data = load_pretrain_data(\n",
    "#         args.data_dir,\n",
    "#         f'patient_sequences/{args.sequence_file}',\n",
    "#         f'patient_id_dict/{args.id_file}',\n",
    "# )\n",
    "# train_dataset = PretrainDatasetDecoder(\n",
    "#         data=pre_data,\n",
    "#         tokenizer=tokenizer,\n",
    "#         max_len=args.max_len,\n",
    "# )\n",
    "\n",
    "\n",
    "_, fine_test = load_finetune_data(\n",
    "    args.data_dir, args.sequence_file, args.id_file, \"few_shot\", \"all\"\n",
    ")\n",
    "test_dataset = PretrainDatasetDecoder(\n",
    "    data=fine_test,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=args.max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(20600, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=20600, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = MambaConfig(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    hidden_size=768,\n",
    "    state_size=16,\n",
    "    num_hidden_layers=32,\n",
    "    max_seq_length=2048,\n",
    "    pad_token_id=tokenizer.get_pad_token_id(),\n",
    "    bos_token_id=tokenizer.token_to_id(\"[CLS]\"),\n",
    "    eos_token_id=tokenizer.get_pad_token_id(),\n",
    ")\n",
    "\n",
    "# embeddings = MambaEmbeddingsForCEHR(\n",
    "#     config=config\n",
    "# )\n",
    "\n",
    "model = MambaForCausalLM(config=config)\n",
    "# model.backbone.embeddings = embeddings\n",
    "model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(20600, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=20600, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "checkpoint = torch.load(\"checkpoints/mamba_pretrain/best.ckpt\", map_location=device)\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(state_dict)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concept_ids': tensor([[   5,    3, 3637,  ...,    0,    0,    0]], device='cuda:0'),\n",
       " 'labels': tensor([[   5,    3, 3637,  ...,    0,    0,    0]], device='cuda:0')}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    test_dataset,  # train_dataset\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "sample = test_dataset[97]  # train_dataset[0]\n",
    "sample = {key: tensor.unsqueeze(0).to(device) for key, tensor in sample.items()}\n",
    "\n",
    "# sample = next(iter(train_loader))\n",
    "# sample = {key:tensor.to(device) for key, tensor in sample.items()}\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [VS] 0FC78ZZ 0FC98ZZ 49281041688 00338011704 00641607825 51484_2 51491_3 51498_2 52010_3 52009_4 52008_0 52007_2 52004_4 52005_3 52006_1 51250_0 51265_3 51277_0 51279_4 51301_3 50861_4 50863_3 50868_3 50878_3 50882_1 50883_4 50884_3 50885_4 50893_4 50902_2 50912_2 50931_2 50960_3 50970_2 50971_2 50983_3 51006_0 51221_4 51222_4 51248_3 50912_1 50931_2 50971_2 50868_2 50983_3 51006_0 50878_2 50882_3 51237_2 50861_4 51274_1 50863_3 50885_4 50902_1 [VE] [REG]\n"
     ]
    }
   ],
   "source": [
    "input_ids = sample[\"concept_ids\"].squeeze().tolist()\n",
    "input_ids = input_ids[: input_ids.index(0)]\n",
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50878_2 50882_3 51237_2 50861_4 51274_1 50863_3 50885_4 50902_1 [VE] [REG]'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50882_1 50885_3 50902_3 51221_4 51222_4 51248_1 51250_0 51265_3 51277_0 51279_4'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(\n",
    "    torch.tensor(input_ids[:-10], dtype=torch.int32).unsqueeze(0).to(device),\n",
    "    max_new_tokens=10,\n",
    ")\n",
    "\n",
    "tokenizer.decode(output.squeeze().tolist()[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, MambaForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "# model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "\n",
    "# inputs = tokenizer([\"Hello, my dog is cute\", \"NO\", \"Go to Sumeru\"], padding=True, return_tensors=\"pt\")\n",
    "# outputs = model(inputs['input_ids'], labels=inputs[\"input_ids\"])\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "# model.backbone.embeddings.cache_input(\n",
    "#     token_type_ids_batch = sample['type_ids'],\n",
    "#     position_ids_batch = None,\n",
    "#     inputs_embeds = None,\n",
    "#     time_stamps = sample['time_stamps'],\n",
    "#     ages = sample['ages'],\n",
    "#     visit_orders = sample['visit_orders'],\n",
    "#     visit_segments = sample['visit_segments']\n",
    "# )\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=sample[\"concept_ids\"], labels=sample[\"concept_ids\"], return_dict=True\n",
    ")\n",
    "\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaPretrain(pl.LightningModule):\n",
    "    \"\"\"Mamba model for pretraining.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_size: int = 768,\n",
    "        state_size: int = 16,\n",
    "        num_hidden_layers: int = 32,\n",
    "        expand: int = 2,\n",
    "        conv_kernel: int = 4,\n",
    "        learning_rate: float = 5e-5,\n",
    "        dropout_prob: float = 0.1,\n",
    "        padding_idx: int = 0,\n",
    "        cls_idx: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.state_size = state_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.expand = expand\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.padding_idx = padding_idx\n",
    "        self.cls_idx = cls_idx\n",
    "\n",
    "        self.config = MambaConfig(\n",
    "            vocab_size=self.vocab_size,\n",
    "            hidden_size=self.embedding_size,\n",
    "            state_size=self.state_size,\n",
    "            num_hidden_layers=self.num_hidden_layers,\n",
    "            expand=self.expand,\n",
    "            conv_kernel=self.conv_kernel,\n",
    "            pad_token_id=self.padding_idx,\n",
    "            bos_token_id=self.cls_idx,\n",
    "            eos_token_id=self.padding_idx,\n",
    "        )\n",
    "\n",
    "        self.model = MambaForCausalLM(config=config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "    ) -> Union[Tuple[torch.Tensor, ...], MambaOutput]:\n",
    "        \"\"\"Forward pass for the model.\"\"\"\n",
    "\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            labels=input_ids,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch: Dict[str, Any], batch_idx: int) -> Any:\n",
    "        \"\"\"Train model on training dataset.\"\"\"\n",
    "        concept_ids = batch[\"concept_ids\"]\n",
    "\n",
    "        # Ensure use of mixed precision\n",
    "        with autocast():\n",
    "            loss = self(\n",
    "                concept_ids,\n",
    "                return_dict=True,\n",
    "            ).loss\n",
    "\n",
    "        (current_lr,) = self.lr_schedulers().get_last_lr()\n",
    "        self.log_dict(\n",
    "            dictionary={\"train_loss\": loss, \"lr\": current_lr},\n",
    "            on_step=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, Any], batch_idx: int) -> Any:\n",
    "        \"\"\"Evaluate model on validation dataset.\"\"\"\n",
    "        concept_ids = batch[\"concept_ids\"]\n",
    "\n",
    "        # Ensure use of mixed precision\n",
    "        with autocast():\n",
    "            loss = self(\n",
    "                concept_ids,\n",
    "                return_dict=True,\n",
    "            ).loss\n",
    "\n",
    "        (current_lr,) = self.lr_schedulers().get_last_lr()\n",
    "        self.log_dict(\n",
    "            dictionary={\"val_loss\": loss, \"lr\": current_lr},\n",
    "            on_step=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> Tuple[list[Any], list[dict[str, SequentialLR | str]]]:\n",
    "        \"\"\"Configure optimizers and learning rate scheduler.\"\"\"\n",
    "        optimizer = optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "        )\n",
    "\n",
    "        n_steps = self.trainer.estimated_stepping_batches\n",
    "        n_warmup_steps = int(0.1 * n_steps)\n",
    "        n_decay_steps = int(0.9 * n_steps)\n",
    "\n",
    "        warmup = LinearLR(\n",
    "            optimizer,\n",
    "            start_factor=0.01,\n",
    "            end_factor=1.0,\n",
    "            total_iters=n_warmup_steps,\n",
    "        )\n",
    "        decay = LinearLR(\n",
    "            optimizer,\n",
    "            start_factor=1.0,\n",
    "            end_factor=0.01,\n",
    "            total_iters=n_decay_steps,\n",
    "        )\n",
    "        scheduler = SequentialLR(\n",
    "            optimizer=optimizer,\n",
    "            schedulers=[warmup, decay],\n",
    "            milestones=[n_warmup_steps],\n",
    "        )\n",
    "\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Emebeddings -> Not now\n",
    "2. Padding order -> Done automatically\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaEmbeddingsForCEHR(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: MambaConfig,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        type_vocab_size: int = 8,\n",
    "        time_embeddings_size: int = 16,\n",
    "        visit_order_size: int = 3,\n",
    "        layer_norm_eps: float = 1e-12,\n",
    "        hidden_dropout_prob: float = 0.1,\n",
    "    ) -> None:\n",
    "        \"\"\"Initiate wrapper class for embeddings used in BigBird CEHR classes.\"\"\"\n",
    "        super().__init__()\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            padding_idx=config.pad_token_id,\n",
    "        )\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            self.max_position_embeddings,\n",
    "            config.hidden_size,\n",
    "        )\n",
    "        self.token_type_embeddings = nn.Embedding(\n",
    "            self.type_vocab_size,\n",
    "            config.hidden_size,\n",
    "        )\n",
    "        self.visit_order_embeddings = nn.Embedding(\n",
    "            self.max_position_embeddings,\n",
    "            config.hidden_size,\n",
    "        )\n",
    "        self.time_embeddings = TimeEmbeddingLayer(\n",
    "            embedding_size=time_embeddings_size,\n",
    "            is_time_delta=True,\n",
    "        )\n",
    "        self.age_embeddings = TimeEmbeddingLayer(\n",
    "            embedding_size=time_embeddings_size,\n",
    "        )\n",
    "        self.visit_segment_embeddings = VisitEmbedding(\n",
    "            visit_order_size=visit_order_size,\n",
    "            embedding_size=config.hidden_size,\n",
    "        )\n",
    "        self.scale_back_concat_layer = nn.Linear(\n",
    "            config.hidden_size + 2 * time_embeddings_size,\n",
    "            config.hidden_size,\n",
    "        )\n",
    "\n",
    "        self.time_stamps: Optional[torch.Tensor] = None\n",
    "        self.ages: Optional[torch.Tensor] = None\n",
    "        self.visit_orders: Optional[torch.Tensor] = None\n",
    "        self.visit_segments: Optional[torch.Tensor] = None\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model\n",
    "        # variable name and be able to load any TensorFlow checkpoint file.\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=self.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory.\n",
    "        self.position_embedding_type = getattr(\n",
    "            config,\n",
    "            \"position_embedding_type\",\n",
    "            \"absolute\",\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.max_position_embeddings).expand((1, -1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"token_type_ids\",\n",
    "            torch.zeros(self.position_ids.size(), dtype=torch.long),\n",
    "            persistent=False,\n",
    "        )\n",
    "        # End copy\n",
    "\n",
    "    def cache_input(\n",
    "        self,\n",
    "        token_type_ids_batch: Optional[torch.Tensor] = None,\n",
    "        position_ids_batch: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        time_stamps: Optional[torch.Tensor] = None,\n",
    "        ages: Optional[torch.Tensor] = None,\n",
    "        visit_orders: Optional[torch.Tensor] = None,\n",
    "        visit_segments: Optional[torch.Tensor] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Cache values for time_stamps, ages, visit_orders & visit_segments.\n",
    "\n",
    "        These values will be used by the forward pass to change the final embedding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        token_type_ids_batch : torch.Tensor\n",
    "            The token type IDs of the input data.\n",
    "        position_ids_batch : torch.Tensor\n",
    "            The position IDs of the input data.\n",
    "        inputs_embeds : torch.Tensor\n",
    "            The embeddings of the input data.\n",
    "        time_stamps : torch.Tensor\n",
    "            Time stamps of the input data.\n",
    "        ages : torch.Tensor\n",
    "            Ages of the input data.\n",
    "        visit_orders : torch.Tensor\n",
    "            Visit orders of the input data.\n",
    "        visit_segments : torch.Tensor\n",
    "            Visit segments of the input data.\n",
    "        \"\"\"\n",
    "        self.token_type_ids_batch = token_type_ids_batch\n",
    "        self.position_ids_batch = position_ids_batch\n",
    "        self.inputs_embeds = inputs_embeds\n",
    "        self.time_stamps = time_stamps\n",
    "        self.ages = ages\n",
    "        self.visit_orders = visit_orders\n",
    "        self.visit_segments = visit_segments\n",
    "\n",
    "    def clear_cache(self) -> None:\n",
    "        \"\"\"Delete the tensors cached by cache_input method.\"\"\"\n",
    "        del (\n",
    "            self.token_type_ids_batch,\n",
    "            self.position_ids_batch,\n",
    "            self.inputs_embeds,\n",
    "            self.time_stamps,\n",
    "            self.ages,\n",
    "            self.visit_orders,\n",
    "            self.visit_segments,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_values_length: int = 0,\n",
    "    ) -> Any:\n",
    "        \"\"\"Return the final embeddings of concept ids using input and cached values.\"\"\"\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = self.inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if self.position_ids_batch is None:\n",
    "            self.position_ids_batch = self.position_ids[\n",
    "                :,\n",
    "                past_key_values_length : seq_length + past_key_values_length,\n",
    "            ]\n",
    "\n",
    "        # Setting the token_type_ids to the registered buffer in constructor\n",
    "        if self.token_type_ids_batch is None:\n",
    "            if hasattr(self, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n",
    "                    input_shape[0],\n",
    "                    seq_length,\n",
    "                )\n",
    "                self.token_type_ids_batch = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                self.token_type_ids_batch = torch.zeros(\n",
    "                    input_shape,\n",
    "                    dtype=torch.long,\n",
    "                    device=self.position_ids.device,\n",
    "                )\n",
    "\n",
    "        if self.inputs_embeds is None:\n",
    "            self.inputs_embeds = self.word_embeddings(input_ids)\n",
    "\n",
    "        # Using cached values from a prior cache_input call\n",
    "        time_stamps_embeds = self.time_embeddings(self.time_stamps)\n",
    "        ages_embeds = self.age_embeddings(self.ages)\n",
    "        visit_segments_embeds = self.visit_segment_embeddings(self.visit_segments)\n",
    "        visit_order_embeds = self.visit_order_embeddings(self.visit_orders)\n",
    "\n",
    "        position_embeds = self.position_embeddings(self.position_ids_batch)\n",
    "        token_type_embeds = self.token_type_embeddings(self.token_type_ids_batch)\n",
    "\n",
    "        self.inputs_embeds = torch.cat(\n",
    "            (self.inputs_embeds, time_stamps_embeds, ages_embeds),\n",
    "            dim=-1,\n",
    "        )\n",
    "        print(self.inputs_embeds.shape)\n",
    "        self.inputs_embeds = self.tanh(self.scale_back_concat_layer(self.inputs_embeds))\n",
    "        embeddings = self.inputs_embeds + token_type_embeds\n",
    "        embeddings += position_embeds\n",
    "        embeddings += visit_order_embeds\n",
    "        embeddings += visit_segments_embeds\n",
    "\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "\n",
    "        # Clear the cache for next forward call\n",
    "        self.clear_cache()\n",
    "\n",
    "        return embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
