{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mamba model.\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "\n",
    "from transformers import BigBirdConfig, AutoModel, AutoTokenizer\n",
    "from transformers.modeling_outputs import MaskedLMOutput, SequenceClassifierOutput\n",
    "from transformers.models.mamba.modeling_mamba import (\n",
    "    MambaConfig,\n",
    "    MambaModel,\n",
    "    MambaForCausalLM,\n",
    "    MambaPreTrainedModel,\n",
    "    MambaOutput,\n",
    "    MAMBA_START_DOCSTRING,\n",
    "    MAMBA_INPUTS_DOCSTRING\n",
    ")\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "ROOT = '/h/afallah/odyssey/odyssey'\n",
    "_CHECKPOINT_FOR_DOC = \"state-spaces/mamba-130m-hf\"\n",
    "_CONFIG_FOR_DOC = \"MambaConfig\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "from odyssey.models.embeddings import BigBirdEmbeddingsForCEHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "input_ids = tokenizer(\"Hey how are you doing?\", return_tensors= \"pt\")[\"input_ids\"]\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2  # the number of labels\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "model = MambaLMHeadModel()\n",
    "\n",
    "model.lm_head = torch.nn.Linear(model.config.d_model, num_labels)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = MambaPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = MambaLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    The MAMBA Model transformer with a language modeling head on top (linear layer with weights tied to the input\n",
    "    embeddings).\n",
    "    \"\"\",\n",
    "    MAMBA_START_DOCSTRING,\n",
    ")\n",
    "class MambaForMaskedLM(MambaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.backbone = MambaModel(config)\n",
    "        self.lm_head = MambaOnlyMLMHead(config) # nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "    \n",
    "    def get_input_embeddings(self):\n",
    "        return self.backbone.get_input_embeddings()\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        return self.backbone.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def _update_model_kwargs_for_generation(\n",
    "        self, outputs: ModelOutput, model_kwargs: Dict[str, Any], **kwargs\n",
    "    ) -> Dict[str, Any]:\n",
    "        model_kwargs[\"cache_params\"] = outputs.get(\"cache_params\", None)\n",
    "        return model_kwargs\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(MAMBA_INPUTS_DOCSTRING)\n",
    "    @add_code_sample_docstrings(\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=MambaMaskedLMOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[MaskedLMOutput, Tuple[torch.FloatTensor]]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n",
    "            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> import torch\n",
    "        >>> from transformers import AutoTokenizer, BigBirdForMaskedLM\n",
    "        >>> from datasets import load_dataset\n",
    "\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "        >>> model = BigBirdForMaskedLM.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "        >>> squad_ds = load_dataset(\"squad_v2\", split=\"train\")  # doctest: +IGNORE_RESULT\n",
    "\n",
    "        >>> # select random long article\n",
    "        >>> LONG_ARTICLE_TARGET = squad_ds[81514][\"context\"]\n",
    "        >>> # select random sentence\n",
    "        >>> LONG_ARTICLE_TARGET[332:398]\n",
    "        'the highest values are very close to the theoretical maximum value'\n",
    "\n",
    "        >>> # add mask_token\n",
    "        >>> LONG_ARTICLE_TO_MASK = LONG_ARTICLE_TARGET.replace(\"maximum\", \"[MASK]\")\n",
    "        >>> inputs = tokenizer(LONG_ARTICLE_TO_MASK, return_tensors=\"pt\")\n",
    "        >>> # long article input\n",
    "        >>> list(inputs[\"input_ids\"].shape)\n",
    "        [1, 919]\n",
    "\n",
    "        >>> with torch.no_grad():\n",
    "        ...     logits = model(**inputs).logits\n",
    "        >>> # retrieve index of [MASK]\n",
    "        >>> mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        >>> predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "        >>> tokenizer.decode(predicted_token_id)\n",
    "        'maximum'\n",
    "        ```\n",
    "\n",
    "        ```python\n",
    "        >>> labels = tokenizer(LONG_ARTICLE_TARGET, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        >>> labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "        >>> outputs = model(**inputs, labels=labels)\n",
    "        >>> round(outputs.loss.item(), 2)\n",
    "        1.99\n",
    "        ```\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        effective_batch_size = input_shape[0]\n",
    "\n",
    "        #  add a dummy token\n",
    "        if self.config.pad_token_id is None:\n",
    "            raise ValueError(\"The PAD token should be defined for generation\")\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_zeros((attention_mask.shape[0], 1))], dim=-1)\n",
    "        dummy_token = torch.full(\n",
    "            (effective_batch_size, 1), self.config.pad_token_id, dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        input_ids = torch.cat([input_ids, dummy_token], dim=1)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
