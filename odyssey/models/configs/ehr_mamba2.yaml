model:
  embedding_size: 768
  max_seq_length: 2048
  max_num_visits: 512
  state_size: 64 ?
  num_heads: 64 ?
  head_dim: 32 ? 
  num_hidden_layers: 32 ?
  expand: 2
  conv_kernel: 4
  dropout_prob: 0.1
  learning_rate: 5.e-5
  n_groups: 8 ? 
  chunk_size: 256 ?

train:
  batch_size: 12  # 44
  num_workers: 3  # 5
  gpus: 1  # 4
  nodes: 1
  max_epochs: 2  # 15
  acc: 1
  persistent_workers: True
  pin_memory: False

finetune:
  batch_size: 64  # 26
  num_workers: 3  # 5
  gpus: 1  # 4
  nodes: 1
  max_epochs: 3  # 6
  acc: 1
  patience: 10
  persistent_workers: True
  pin_memory: False