{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "ROOT = \"/fs01/home/afallah/odyssey/slurm\"\n",
    "os.chdir(ROOT)\n",
    "# from models.cehr_bert.tokenizer import ConceptTokenizer\n",
    "import glob\n",
    "import json\n",
    "from itertools import chain\n",
    "from random import randint\n",
    "from typing import Sequence, Union\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers\n",
    "\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_ROOT = f\"{ROOT}/data\"\n",
    "DATA_PATH = f\"{DATA_ROOT}/patient_sequences.parquet\"\n",
    "special_tokens = (\n",
    "    [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[REG]\", \"[MASK]\", \"[VS]\", \"[VE]\"]\n",
    "    + [f\"[W_{i}]\" for i in range(0, 4)]\n",
    "    + [f\"[M_{i}]\" for i in range(0, 13)]\n",
    "    + [\"[LT]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    seed = 23\n",
    "    data_dir = DATA_ROOT\n",
    "    test_size = 0.2\n",
    "    batch_size = 64\n",
    "    num_workers = 2\n",
    "    vocab_size = None\n",
    "    embedding_size = 128\n",
    "    time_embeddings_size = 16\n",
    "    max_len = 512\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# data = pd.read_parquet(DATA_PATH)\n",
    "# data.rename(columns={'event_tokens': 'event_tokens_untruncated', 'event_tokens_updated': 'event_tokens'}, inplace=True)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define random dataset using the actual vocabulary ###\n",
    "lab_dict = [\n",
    "    \"50995_4\",\n",
    "    \"51429_0\",\n",
    "    \"51009_4\",\n",
    "    \"50908_4\",\n",
    "    \"51564_0\",\n",
    "    \"52301_3\",\n",
    "    \"51044_3\",\n",
    "    \"50841_4\",\n",
    "    \"52178_0\",\n",
    "    \"51512_0\",\n",
    "]\n",
    "procedure_dict = [\n",
    "    \"00641601310\",\n",
    "    \"00069419068\",\n",
    "    \"00078037742\",\n",
    "    \"63323027205\",\n",
    "    \"00078059620\",\n",
    "    \"00007550040\",\n",
    "    \"51079017220\",\n",
    "    \"68084061221\",\n",
    "]\n",
    "diagnosis_dict = [\n",
    "    \"02H64JZ\",\n",
    "    \"7906\",\n",
    "    \"8222\",\n",
    "    \"0JC00ZZ\",\n",
    "    \"0YHM0YZ\",\n",
    "    \"9652\",\n",
    "    \"03743D6\",\n",
    "    \"100\",\n",
    "    \"0B998ZZ\",\n",
    "    \"8127\",\n",
    "]\n",
    "time_dict = [\"[W_1]\", \"[W_3]\", \"[M_2]\", \"[M_5]\", \"[LT]\"]\n",
    "\n",
    "\n",
    "def generate_random_events(num_visits, event_dict, time_tokens):\n",
    "    patient = []\n",
    "    length_visits = [randint(1, 6) for _ in range(num_visits)]\n",
    "\n",
    "    for i in range(num_visits):\n",
    "        patient.append(\"[VS]\")\n",
    "        length_visit = length_visits[i]\n",
    "        random_events = [\n",
    "            event_dict[randint(0, len(event_dict) - 1)] for _ in range(length_visit)\n",
    "        ]\n",
    "        patient += random_events\n",
    "        patient.append(\"[VE]\")\n",
    "\n",
    "        if i < num_visits - 1:\n",
    "            patient.append(time_tokens[i])\n",
    "\n",
    "        patient.append(\"[REG]\")\n",
    "\n",
    "    return patient\n",
    "\n",
    "\n",
    "def generate_random_patient(lab_dict, procedure_dict, diagnosis_dict, time_dict):\n",
    "    num_visits = randint(1, 5)\n",
    "    time_tokens = [time_dict[randint(0, len(time_dict) - 1)] for _ in range(num_visits)]\n",
    "\n",
    "    random_lab = generate_random_events(num_visits, lab_dict, time_tokens)\n",
    "    random_procedure = generate_random_events(num_visits, procedure_dict, time_tokens)\n",
    "    random_diagnosis = [\"[CLS]\"] + generate_random_events(\n",
    "        num_visits,\n",
    "        diagnosis_dict,\n",
    "        time_tokens,\n",
    "    )\n",
    "\n",
    "    prior_vs_diagnosis = [\n",
    "        diagnosis_dict[randint(0, len(diagnosis_dict) - 1)]\n",
    "        for _ in range(randint(0, 5))\n",
    "    ]\n",
    "    random_diagnosis = [random_diagnosis[0]] + prior_vs_diagnosis + random_diagnosis[1:]\n",
    "\n",
    "    return {\n",
    "        \"diagnosis\": random_diagnosis,\n",
    "        \"procedure\": random_procedure,\n",
    "        \"lab\": random_lab,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_random_dataset(\n",
    "    lab_dict,\n",
    "    procedure_dict,\n",
    "    diagnosis_dict,\n",
    "    time_dict,\n",
    "    num_patients=10,\n",
    "):\n",
    "    patients = []\n",
    "\n",
    "    for i in range(num_patients):\n",
    "        patient = generate_random_patient(\n",
    "            lab_dict,\n",
    "            procedure_dict,\n",
    "            diagnosis_dict,\n",
    "            time_dict,\n",
    "        )\n",
    "        patients.append(patient)\n",
    "\n",
    "    return patients\n",
    "\n",
    "\n",
    "# Assume these are already truncated\n",
    "patients = generate_random_dataset(lab_dict, procedure_dict, diagnosis_dict, time_dict)\n",
    "patients = pd.DataFrame(patients)\n",
    "patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical Alignment For Single Example\n",
    "patient = patients.iloc[9]\n",
    "\n",
    "procedure_ref = []\n",
    "next_iter_procedure_ref = []\n",
    "new_procedure = [[]]\n",
    "\n",
    "d = 0\n",
    "p = 0\n",
    "while patient[\"diagnosis\"][d] != \"[VS]\":\n",
    "    new_procedure[-1].append(\"[PAD]\")\n",
    "    d += 1\n",
    "\n",
    "\n",
    "while d < len(patient[\"diagnosis\"]):\n",
    "    if patient[\"procedure\"][p] == \"[VE]\" and patient[\"diagnosis\"][d] != \"[VE]\":\n",
    "        new_procedure[-1].append(\"[PAD]\")\n",
    "        d += 1\n",
    "        continue\n",
    "\n",
    "    elif patient[\"procedure\"][p] != \"[VE]\" and patient[\"diagnosis\"][d] == \"[VE]\":\n",
    "        vs_index = len(new_procedure[-1]) - new_procedure[-1][::-1].index(\"[VS]\") - 1\n",
    "        procedure_ref.append((p, vs_index))\n",
    "\n",
    "        while patient[\"procedure\"][p] != \"[VE]\":\n",
    "            p += 1\n",
    "\n",
    "    new_procedure[-1].append(patient[\"procedure\"][p])\n",
    "    d += 1\n",
    "    p += 1\n",
    "\n",
    "\n",
    "while procedure_ref or next_iter_procedure_ref:\n",
    "    if next_iter_procedure_ref:\n",
    "        procedure_ref = next_iter_procedure_ref.copy()\n",
    "        next_iter_procedure_ref = []\n",
    "\n",
    "    new_procedure.append([])\n",
    "\n",
    "    for i, (ref, vs_index) in enumerate(procedure_ref):\n",
    "        if len(new_procedure[-1]) == 0:\n",
    "            n = 0\n",
    "            while n <= vs_index:\n",
    "                current_token = new_procedure[0][n]\n",
    "                if current_token in special_tokens:\n",
    "                    new_procedure[-1].append(current_token)\n",
    "                else:\n",
    "                    new_procedure[-1].append(\"[PAD]\")\n",
    "                n += 1\n",
    "\n",
    "        n = vs_index + 1\n",
    "        p = ref\n",
    "\n",
    "        while patient[\"procedure\"][p] != \"[VE]\" and new_procedure[0][n] != \"[VE]\":\n",
    "            new_procedure[-1].append(patient[\"procedure\"][p])\n",
    "            n += 1\n",
    "            p += 1\n",
    "\n",
    "        if patient[\"procedure\"][p] != \"[VE]\" and new_procedure[0][n] == \"[VE]\":\n",
    "            next_iter_procedure_ref.append((p, vs_index))\n",
    "\n",
    "        procedure_ref.remove((ref, vs_index))\n",
    "\n",
    "        if len(procedure_ref) == 0:\n",
    "            next_vs_index = len(new_procedure[0]) - 1\n",
    "        else:\n",
    "            next_vs_index = procedure_ref[i][1]\n",
    "\n",
    "        while n <= next_vs_index:\n",
    "            current_token = new_procedure[0][n]\n",
    "            if current_token in special_tokens:\n",
    "                new_procedure[-1].append(current_token)\n",
    "            else:\n",
    "                new_procedure[-1].append(\"[PAD]\")\n",
    "            n += 1\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Diagnosis:\\n{patient['diagnosis']} \\n\\nOld Procedure:\\n{patient['procedure']} \\n\\nNew Procedure:\\n{new_procedure}\\n\",\n",
    ")\n",
    "print(f\"Len check: {len(new_procedure[0]) == len(patient['diagnosis'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {}\n",
    "\n",
    "vocab_json_files = glob.glob(os.path.join(config.data_dir, \"*_vocab.json\"))\n",
    "for file in vocab_json_files:\n",
    "    vocab = json.load(open(file, \"r\"))\n",
    "\n",
    "    vocab_type = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    vocab_dict[vocab_type] = vocab\n",
    "\n",
    "combined_vocab = list(chain.from_iterable(list(vocab_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vocab = special_tokens + combined_vocab\n",
    "tokenizer_vocab = {token: i for i, token in enumerate(combined_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_object = Tokenizer(\n",
    "    models.WordPiece(\n",
    "        vocab=tokenizer_vocab,\n",
    "        unk_token=\"[UNK]\",\n",
    "        max_input_chars_per_word=1000,\n",
    "    ),\n",
    ")\n",
    "tokenizer_object.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "example = \" \".join(patients.iloc[5][\"diagnosis\"] + [\"[UNK] [PAD] [PAD] [PAD] [PAD]\"])\n",
    "example2 = \" \".join(patients.iloc[1][\"lab\"] + [\"[UNK] [PAD] [PAD]\"])\n",
    "encoding = tokenizer_object.decode([0, 1, 2])\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer_object,\n",
    "    bos_token=\"[VS]\",\n",
    "    eos_token=\"[VE]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    # sep_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "tokenizer(\n",
    "    [example, example2],\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=True,\n",
    "    truncation=False,\n",
    "    padding=True,\n",
    "    max_length=2048,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptTokenizer:\n",
    "    \"\"\"Tokenizer for event concepts.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pad_token: str = \"[PAD]\",\n",
    "        mask_token: str = \"[MASK]\",\n",
    "        start_token: str = \"[VS]\",\n",
    "        end_token: str = \"[VE]\",\n",
    "        oov_token=\"-1\",\n",
    "        data_dir: str = \"data_files\",\n",
    "    ):\n",
    "        self.tokenizer = Tokenizer(oov_token=oov_token, filters=\"\", lower=False)\n",
    "        self.mask_token = mask_token\n",
    "        self.pad_token = pad_token\n",
    "        self.special_tokens = (\n",
    "            [pad_token, mask_token, start_token, end_token]\n",
    "            + [f\"W_{i}\" for i in range(0, 4)]\n",
    "            + [f\"M_{i}\" for i in range(0, 13)]\n",
    "            + [\"LT\"]\n",
    "        )\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def fit_on_vocab(self) -> None:\n",
    "        \"\"\"Fit the tokenizer on the vocabulary.\"\"\"\n",
    "        vocab_json_files = glob.glob(os.path.join(self.data_dir, \"*_vocab.json\"))\n",
    "        for file in vocab_json_files:\n",
    "            vocab = json.load(open(file, \"r\"))\n",
    "            self.tokenizer.fit_on_texts(vocab)\n",
    "        self.tokenizer.fit_on_texts(self.special_tokens)\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        concept_sequences: Union[str, Sequence[str]],\n",
    "        is_generator: bool = False,\n",
    "    ) -> Union[int, Sequence[int]]:\n",
    "        \"\"\"Encode the concept sequences into token ids.\"\"\"\n",
    "        return (\n",
    "            self.tokenizer.texts_to_sequences_generator(concept_sequences)\n",
    "            if is_generator\n",
    "            else self.tokenizer.texts_to_sequences(concept_sequences)\n",
    "        )\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        concept_sequence_token_ids: Union[int, Sequence[int]],\n",
    "    ) -> Sequence[str]:\n",
    "        \"\"\"Decode the concept sequence token ids into concepts.\"\"\"\n",
    "        return self.tokenizer.sequences_to_texts(concept_sequence_token_ids)\n",
    "\n",
    "    def get_all_token_indexes(self) -> set:\n",
    "        all_keys = set(self.tokenizer.index_word.keys())\n",
    "\n",
    "        if self.tokenizer.oov_token is not None:\n",
    "            all_keys.remove(self.tokenizer.word_index[self.tokenizer.oov_token])\n",
    "\n",
    "        if self.special_tokens is not None:\n",
    "            excluded = set(\n",
    "                [\n",
    "                    self.tokenizer.word_index[special_token]\n",
    "                    for special_token in self.special_tokens\n",
    "                ],\n",
    "            )\n",
    "            all_keys = all_keys - excluded\n",
    "        return all_keys\n",
    "\n",
    "    def get_first_token_index(self) -> int:\n",
    "        return min(self.get_all_token_indexes())\n",
    "\n",
    "    def get_last_token_index(self) -> int:\n",
    "        return max(self.get_all_token_indexes())\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        # + 1 because oov_token takes the index 0\n",
    "        return len(self.tokenizer.index_word) + 1\n",
    "\n",
    "    def get_pad_token_id(self):\n",
    "        pad_token_id = self.encode(self.pad_token)\n",
    "        while isinstance(pad_token_id, list):\n",
    "            pad_token_id = pad_token_id[0]\n",
    "        return pad_token_id\n",
    "\n",
    "    def get_mask_token_id(self):\n",
    "        mask_token_id = self.encode(self.mask_token)\n",
    "        while isinstance(mask_token_id, list):\n",
    "            mask_token_id = mask_token_id[0]\n",
    "        return mask_token_id\n",
    "\n",
    "    def get_special_token_ids(self):\n",
    "        special_ids = self.encode(self.special_tokens)\n",
    "        flat_special_ids = [item[0] for item in special_ids]\n",
    "        return flat_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ConceptTokenizer(data_dir=config.data_dir)\n",
    "tokenizer.fit_on_vocab()\n",
    "config.vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([[0, 1, 2, 3, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode([\"[PAD] [UNKoieri] [CLS] [VS]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(patients.iloc[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
