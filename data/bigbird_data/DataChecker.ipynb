{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T22:07:54.129529900Z",
     "start_time": "2024-03-01T22:07:52.645804Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from typing import Dict, Any, List\n",
    "from os.path import join\n",
    "\n",
    "random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T22:14:25.197103500Z",
     "start_time": "2024-03-01T22:14:25.085543700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['100_patients', '500_patients', '1000_patients', '5000_patients', '20000_patients'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_ids = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_1month.pkl'), 'rb'))\n",
    "patient_ids['valid']['few_shot'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T21:09:11.746695300Z",
     "start_time": "2024-02-28T21:09:11.327956700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# patient_ids2 = pickle.load(open(join(\"/h/afallah/odyssey/odyssey/data/bigbird_data\", 'dataset_2048_mortality_2weeks.pkl'), 'rb'))['pretrain']\n",
    "#\n",
    "# patient_ids1.sort()\n",
    "# patient_ids2.sort()\n",
    "#\n",
    "# patient_ids1 == patient_ids2\n",
    "# # dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids['pretrain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_2048 = pd.read_parquet('patient_sequences_2048.parquet')\n",
    "# dataset_2048.sort_values(by='patient_id', inplace=True)\n",
    "dataset_2048.drop(['event_tokens', 'type_tokens', 'age_tokens', 'time_tokens', 'visit_tokens', 'position_tokens'], axis=1, inplace=True)\n",
    "\n",
    "dataset_2048['event_tokens_2048'] = dataset_2048['event_tokens_2048'].transform(lambda token_list: ' '.join(token_list))\n",
    "dataset_2048['label_mortality_2weeks'] = ((dataset_2048['death_after_start'] >= 0) & (dataset_2048['death_after_end'] <= 15)).astype(int)\n",
    "dataset_2048['label_mortality_1month'] = ((dataset_2048['death_after_start'] >= 0) & (dataset_2048['death_after_end'] <= 32)).astype(int)\n",
    "\n",
    "dataset_2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-01T22:07:24.016011600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_2048 = pd.read_parquet('patient_sequences_2048_labeled.parquet')\n",
    "dataset_2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T21:07:35.772425Z",
     "start_time": "2024-02-28T21:07:33.692304900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_dataset_train_test_valid_datasets(\n",
    "        dataset: pd.DataFrame,\n",
    "        label_col: str,\n",
    "        cv_size: int,\n",
    "        test_size: int,\n",
    "        finetune_size: List[int],\n",
    "        num_splits: int,\n",
    "        save_path: str) -> Dict[str, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and cross-validation sets using k-fold cross-validation while ensuring balanced label distribution in each fold.\n",
    "    Saves the resulting dictionary to disk.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): The input dataset.\n",
    "        label_col (str): The name of the column containing the labels.\n",
    "        cv_size (int): The number of patients in each cross-validation split.\n",
    "        test_size (int): The number of patients in the test set.\n",
    "        finetune_size (List[int]): The number of patients in each fine-tune set\n",
    "        num_splits (int): The number of splits to create (k value).\n",
    "        save_path (str): The path to save the resulting dictionary.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, List[str]]]: A dictionary containing patient IDs for each split group.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to hold patient IDs for different sets\n",
    "    patient_ids_dict = {'pretrain': [], 'valid': {'few_shot': {}, 'kfold': {}}, 'test': []}\n",
    "\n",
    "    # Sample test patients and remove them from dataset\n",
    "    if test_size > 0:\n",
    "        test_patients = dataset.sample(n=test_size, random_state=23)\n",
    "        dataset.drop(test_patients.index, inplace=True)\n",
    "        patient_ids_dict['test'] = test_patients['patient_id'].tolist()\n",
    "\n",
    "    # Any remaining data is used for pretraining\n",
    "    patient_ids_dict['pretrain'] = dataset['patient_id'].tolist()\n",
    "    random.shuffle(patient_ids_dict['pretrain'])\n",
    "\n",
    "    # few_shot finetune dataset\n",
    "    for each_finetune_size in finetune_size:\n",
    "        subset_size = each_finetune_size // 2\n",
    "\n",
    "        # Sampling positive and negative patients\n",
    "        pos_patients = dataset[dataset[label_col] == True].sample(n=subset_size, random_state=23)\n",
    "        neg_patients = dataset[dataset[label_col] == False].sample(n=subset_size, random_state=23)\n",
    "\n",
    "        # Extracting patient IDs\n",
    "        pos_patients_ids = pos_patients['patient_id'].tolist()\n",
    "        neg_patients_ids = neg_patients['patient_id'].tolist()\n",
    "\n",
    "        # Combining and shuffling patient IDs\n",
    "        finetune_patients = pos_patients_ids + neg_patients_ids\n",
    "        random.shuffle(finetune_patients)\n",
    "        patient_ids_dict['valid']['few_shot'][f'{each_finetune_size}_patients'] = finetune_patients\n",
    "\n",
    "    # Performing stratified k-fold split\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=23)\n",
    "\n",
    "    for i, (train_index, cv_index) in enumerate(skf.split(dataset, dataset[label_col])):\n",
    "\n",
    "        dataset_cv = dataset.iloc[cv_index]\n",
    "        dataset_finetune = dataset.iloc[train_index]\n",
    "\n",
    "        # Separate positive and negative labeled patients\n",
    "        pos_patients = dataset_cv[dataset_cv[label_col] == True]['patient_id'].tolist()\n",
    "        neg_patients = dataset_cv[dataset_cv[label_col] == False]['patient_id'].tolist()\n",
    "\n",
    "        # Calculate the number of positive and negative patients needed for balanced CV set\n",
    "        num_pos_needed = cv_size // 2\n",
    "        num_neg_needed = cv_size // 2\n",
    "\n",
    "        # Select positive and negative patients for CV set ensuring balanced distribution\n",
    "        cv_patients = pos_patients[:num_pos_needed] + neg_patients[:num_neg_needed]\n",
    "        remaining_finetune_patients = pos_patients[num_pos_needed:] + neg_patients[num_neg_needed:]\n",
    "\n",
    "        # Extract patient IDs for training set\n",
    "        finetune_patients = dataset_finetune['patient_id'].tolist()\n",
    "        finetune_patients += remaining_finetune_patients\n",
    "\n",
    "        # Shuffle each list of patients\n",
    "        random.shuffle(cv_patients)\n",
    "        random.shuffle(finetune_patients)\n",
    "\n",
    "        patient_ids_dict['valid']['kfold'][f'group{i+1}'] = {'finetune': finetune_patients, 'cv': cv_patients}\n",
    "\n",
    "    # Save the dictionary to disk\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(patient_ids_dict, f)\n",
    "\n",
    "    return patient_ids_dict\n",
    "\n",
    "\n",
    "patient_ids_dict = split_dataset_train_test_valid_datasets(\n",
    "                                               dataset=dataset_2048,\n",
    "                                               label_col='label_mortality_1month',\n",
    "                                               cv_size=4000,\n",
    "                                               test_size=20000,\n",
    "                                               finetune_size=[100, 500, 1000, 5000, 20000],\n",
    "                                               num_splits=5,\n",
    "                                               save_path='dataset_2048_mortality_1month.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(patient_ids_dict['group3']['cv'])\n",
    "\n",
    "# dataset_2048.loc[dataset_2048['patient_id'].isin(patient_ids_dict['group1']['cv'])]['label_mortality_1month']\n",
    "\n",
    "# s = set()\n",
    "# for i in range(1, 6):\n",
    "#     s = s.union(set(patient_ids_dict[f'group{i}']['cv']))\n",
    "#\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_2048.to_parquet('patient_sequences_2048_labeled.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming dataset.event_tokens is your DataFrame column\n",
    "dataset.event_tokens.transform(len).plot(kind='hist', bins=100)\n",
    "plt.xlim(1000, 8000)  # Limit x-axis to 5000\n",
    "plt.ylim(0, 6000)\n",
    "plt.xlabel('Length of Event Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Event Tokens Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(dataset.event_tokens.transform(len) > 2048)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
